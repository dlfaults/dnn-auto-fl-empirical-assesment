D:\nargiz\github\umlaut\venvUMLT\Scripts\python.exe D:/nargiz/github/umlaut/reuters_change_learning_rate_1.0.py
Using TensorFlow backend.
2023-03-20 07:35:55.634106: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
D:\nargiz\github\umlaut\venvUMLT\lib\site-packages\keras\datasets\reuters.py:85: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])
D:\nargiz\github\umlaut\venvUMLT\lib\site-packages\keras\datasets\reuters.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])
# of Training Samples: 8982
# of Test Samples: 2246
# of Classes: 46
the an brazil vs reuter 68 vs 2 lt index countries 18 000 reuter all number 000 lt oper site consumption 000 reuter considerably reserves 000 an 784 vs reuter holiday vs some 100 hit our forward up indexes share countries 624 writedown index tonnes income a but reuter countries 624 627 a exchange half be 9 dlrs on bond 07 said in all 100 hit our japanese indexes two as before on tonnes said oper half be in hit our japanese pct dlrs
3
[0. 1. 0. ... 0. 0. 0.]
10000
[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
46
7185
mdl model.h5
2023-03-20 07:35:58.453405: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2023-03-20 07:35:58.480244: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:0b:00.0 name: NVIDIA GeForce RTX 2060 SUPER computeCapability: 7.5
coreClock: 1.65GHz coreCount: 34 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s
2023-03-20 07:35:58.480426: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2023-03-20 07:35:58.485209: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2023-03-20 07:35:58.487684: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2023-03-20 07:35:58.488759: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2023-03-20 07:35:58.492046: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2023-03-20 07:35:58.494453: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2023-03-20 07:35:58.500409: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2023-03-20 07:35:58.500536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2023-03-20 07:35:58.501056: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2023-03-20 07:35:58.501755: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:0b:00.0 name: NVIDIA GeForce RTX 2060 SUPER computeCapability: 7.5
coreClock: 1.65GHz coreCount: 34 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s
2023-03-20 07:35:58.501907: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2023-03-20 07:35:58.501993: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2023-03-20 07:35:58.502069: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2023-03-20 07:35:58.502140: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2023-03-20 07:35:58.502215: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2023-03-20 07:35:58.502304: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2023-03-20 07:35:58.502396: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2023-03-20 07:35:58.502493: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2023-03-20 07:35:58.949318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2023-03-20 07:35:58.949413: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2023-03-20 07:35:58.949461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2023-03-20 07:35:58.949610: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6704 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 2060 SUPER, pci bus id: 0000:0b:00.0, compute capability: 7.5)
['loss', 'accuracy']
Train on 7185 samples, validate on 1797 samples
Umlaut results:
[<Critical: Missing Softmax layer before loss>, <Warning: Last model layer has nonlinear activation>]
Epoch 1/3
2023-03-20 07:35:59.733344: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll

  32/7185 [..............................] - ETA: 56s - loss: 3.8561 - accuracy: 0.0000e+00
 384/7185 [>.............................] - ETA: 5s - loss: 895.8437 - accuracy: 0.3750   
 704/7185 [=>............................] - ETA: 3s - loss: 512.7414 - accuracy: 0.3352
1024/7185 [===>..........................] - ETA: 2s - loss: 368.4452 - accuracy: 0.3779
1376/7185 [====>.........................] - ETA: 1s - loss: 287.2039 - accuracy: 0.4026
1728/7185 [======>.......................] - ETA: 1s - loss: 236.3248 - accuracy: 0.4201
2048/7185 [=======>......................] - ETA: 1s - loss: 203.4351 - accuracy: 0.4277
2368/7185 [========>.....................] - ETA: 1s - loss: 182.1599 - accuracy: 0.4274
2688/7185 [==========>...................] - ETA: 1s - loss: 165.0294 - accuracy: 0.4170
3040/7185 [===========>..................] - ETA: 0s - loss: 146.8433 - accuracy: 0.4178
3392/7185 [=============>................] - ETA: 0s - loss: 133.0016 - accuracy: 0.4098
3712/7185 [==============>...............] - ETA: 0s - loss: 128.7588 - accuracy: 0.4062
4032/7185 [===============>..............] - ETA: 0s - loss: 121.1223 - accuracy: 0.4030
4352/7185 [=================>............] - ETA: 0s - loss: 112.6713 - accuracy: 0.4003
4704/7185 [==================>...........] - ETA: 0s - loss: 105.4262 - accuracy: 0.3967
5056/7185 [====================>.........] - ETA: 0s - loss: 99.1398 - accuracy: 0.3912 
5376/7185 [=====================>........] - ETA: 0s - loss: 93.6070 - accuracy: 0.3917
5696/7185 [======================>.......] - ETA: 0s - loss: 91.2259 - accuracy: 0.3929
6016/7185 [========================>.....] - ETA: 0s - loss: 87.6598 - accuracy: 0.3903
6368/7185 [=========================>....] - ETA: 0s - loss: 83.9957 - accuracy: 0.3869
6720/7185 [===========================>..] - ETA: 0s - loss: 80.0042 - accuracy: 0.3847
7040/7185 [============================>.] - ETA: 0s - loss: 77.0571 - accuracy: 0.3817
7185/7185 [==============================] - 2s 219us/step - loss: 75.6476 - accuracy: 0.3822 - val_loss: 9.8907 - val_accuracy: 0.3573

Umlaut results:
[<Warning: Learning Rate is high>]
Epoch 2/3

  32/7185 [..............................] - ETA: 1s - loss: 143.6377 - accuracy: 0.3438
 352/7185 [>.............................] - ETA: 1s - loss: 21.8213 - accuracy: 0.3409 
 704/7185 [=>............................] - ETA: 1s - loss: 184.6132 - accuracy: 0.3366
1056/7185 [===>..........................] - ETA: 0s - loss: 162.4178 - accuracy: 0.3627
1344/7185 [====>.........................] - ETA: 0s - loss: 142.9783 - accuracy: 0.3936
1664/7185 [=====>........................] - ETA: 0s - loss: 155.3168 - accuracy: 0.3918
1952/7185 [=======>......................] - ETA: 0s - loss: 133.6981 - accuracy: 0.3929
2272/7185 [========>.....................] - ETA: 0s - loss: 126.9648 - accuracy: 0.3851
2592/7185 [=========>....................] - ETA: 0s - loss: 115.0473 - accuracy: 0.3816
2912/7185 [===========>..................] - ETA: 0s - loss: 110.1765 - accuracy: 0.3781
3264/7185 [============>.................] - ETA: 0s - loss: 112.2294 - accuracy: 0.3698
3616/7185 [==============>...............] - ETA: 0s - loss: 103.7501 - accuracy: 0.3750
3968/7185 [===============>..............] - ETA: 0s - loss: 95.1798 - accuracy: 0.3740 
4320/7185 [=================>............] - ETA: 0s - loss: 88.3494 - accuracy: 0.3741
4640/7185 [==================>...........] - ETA: 0s - loss: 84.0227 - accuracy: 0.3739
4960/7185 [===================>..........] - ETA: 0s - loss: 80.2343 - accuracy: 0.3688
5312/7185 [=====================>........] - ETA: 0s - loss: 75.8129 - accuracy: 0.3675
5664/7185 [======================>.......] - ETA: 0s - loss: 71.9771 - accuracy: 0.3632
5984/7185 [=======================>......] - ETA: 0s - loss: 88.8439 - accuracy: 0.3636
6336/7185 [=========================>....] - ETA: 0s - loss: 93.0380 - accuracy: 0.3662
6656/7185 [==========================>...] - ETA: 0s - loss: 92.1842 - accuracy: 0.3634
6976/7185 [============================>.] - ETA: 0s - loss: 107.2432 - accuracy: 0.3617
7185/7185 [==============================] - 1s 184us/step - loss: 105.4324 - accuracy: 0.3592 - val_loss: 17.9371 - val_accuracy: 0.4101

Umlaut results:
[<Warning: Learning Rate is high>]
Epoch 3/3

  32/7185 [..............................] - ETA: 1s - loss: 2.2494 - accuracy: 0.5000
 384/7185 [>.............................] - ETA: 1s - loss: 204.1474 - accuracy: 0.3620
 704/7185 [=>............................] - ETA: 1s - loss: 118.5984 - accuracy: 0.3778
1056/7185 [===>..........................] - ETA: 0s - loss: 87.9432 - accuracy: 0.3722 
1376/7185 [====>.........................] - ETA: 0s - loss: 564.2874 - accuracy: 0.3677
1728/7185 [======>.......................] - ETA: 0s - loss: 477.6950 - accuracy: 0.3762
2048/7185 [=======>......................] - ETA: 0s - loss: 425.2483 - accuracy: 0.3774
2368/7185 [========>.....................] - ETA: 0s - loss: 457.9964 - accuracy: 0.3763
2688/7185 [==========>...................] - ETA: 0s - loss: 404.4603 - accuracy: 0.3713
3040/7185 [===========>..................] - ETA: 0s - loss: 363.4173 - accuracy: 0.3661
3392/7185 [=============>................] - ETA: 0s - loss: 326.1633 - accuracy: 0.3588
3712/7185 [==============>...............] - ETA: 0s - loss: 298.5080 - accuracy: 0.3613
4064/7185 [===============>..............] - ETA: 0s - loss: 273.7402 - accuracy: 0.3565
4416/7185 [=================>............] - ETA: 0s - loss: 255.4273 - accuracy: 0.3573
4736/7185 [==================>...........] - ETA: 0s - loss: 240.5479 - accuracy: 0.3552
5056/7185 [====================>.........] - ETA: 0s - loss: 241.4131 - accuracy: 0.3521
5376/7185 [=====================>........] - ETA: 0s - loss: 231.3776 - accuracy: 0.3490
5696/7185 [======================>.......] - ETA: 0s - loss: 218.6011 - accuracy: 0.3464
6048/7185 [========================>.....] - ETA: 0s - loss: 207.0776 - accuracy: 0.3477
6400/7185 [=========================>....] - ETA: 0s - loss: 198.5903 - accuracy: 0.3481
6688/7185 [==========================>...] - ETA: 0s - loss: 190.3851 - accuracy: 0.3473
7008/7185 [============================>.] - ETA: 0s - loss: 182.9787 - accuracy: 0.3463
7185/7185 [==============================] - 1s 183us/step - loss: 183.5072 - accuracy: 0.3473 - val_loss: 44.8261 - val_accuracy: 0.3717

Umlaut results:
[<Warning: Learning Rate is high>]

  32/2246 [..............................] - ETA: 0s
 544/2246 [======>.......................] - ETA: 0s
1088/2246 [=============>................] - ETA: 0s
1632/2246 [====================>.........] - ETA: 0s
2176/2246 [============================>.] - ETA: 0s
2246/2246 [==============================] - 0s 96us/step
Test loss: 42.30185254068111
Test accuracy: 0.3659839630126953
D:\nargiz\github\umlaut\venvUMLT\lib\site-packages\keras\datasets\reuters.py:85: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])
D:\nargiz\github\umlaut\venvUMLT\lib\site-packages\keras\datasets\reuters.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])
# of Training Samples: 8982
# of Test Samples: 2246
# of Classes: 46
the an brazil vs reuter 68 vs 2 lt index countries 18 000 reuter all number 000 lt oper site consumption 000 reuter considerably reserves 000 an 784 vs reuter holiday vs some 100 hit our forward up indexes share countries 624 writedown index tonnes income a but reuter countries 624 627 a exchange half be 9 dlrs on bond 07 said in all 100 hit our japanese indexes two as before on tonnes said oper half be in hit our japanese pct dlrs
3
[0. 1. 0. ... 0. 0. 0.]
10000
[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
46
7185
mdl model.h5
['loss', 'accuracy']
Train on 7185 samples, validate on 1797 samples
Umlaut results:
[<Critical: Missing Softmax layer before loss>, <Warning: Last model layer has nonlinear activation>]
Epoch 1/3

  32/7185 [..............................] - ETA: 16s - loss: 3.8525 - accuracy: 0.0312
 288/7185 [>.............................] - ETA: 2s - loss: 966.3340 - accuracy: 0.3160
 544/7185 [=>............................] - ETA: 2s - loss: 531.5049 - accuracy: 0.3879
 800/7185 [==>...........................] - ETA: 1s - loss: 366.3599 - accuracy: 0.4225
1056/7185 [===>..........................] - ETA: 1s - loss: 288.8518 - accuracy: 0.4299
1312/7185 [====>.........................] - ETA: 1s - loss: 234.8488 - accuracy: 0.4345
1600/7185 [=====>........................] - ETA: 1s - loss: 200.5305 - accuracy: 0.4350
1888/7185 [======>.......................] - ETA: 1s - loss: 172.2204 - accuracy: 0.4322
2208/7185 [========>.....................] - ETA: 1s - loss: 149.0233 - accuracy: 0.4298
2528/7185 [=========>....................] - ETA: 1s - loss: 131.8591 - accuracy: 0.4209
2848/7185 [==========>...................] - ETA: 0s - loss: 117.4081 - accuracy: 0.4098
3168/7185 [============>.................] - ETA: 0s - loss: 106.9563 - accuracy: 0.4040
3488/7185 [=============>................] - ETA: 0s - loss: 97.6576 - accuracy: 0.4028 
3840/7185 [===============>..............] - ETA: 0s - loss: 91.8886 - accuracy: 0.3958
4160/7185 [================>.............] - ETA: 0s - loss: 88.3065 - accuracy: 0.3947
4480/7185 [=================>............] - ETA: 0s - loss: 82.6442 - accuracy: 0.3917
4800/7185 [===================>..........] - ETA: 0s - loss: 78.4462 - accuracy: 0.3910
5152/7185 [====================>.........] - ETA: 0s - loss: 74.6830 - accuracy: 0.3886
5472/7185 [=====================>........] - ETA: 0s - loss: 71.7648 - accuracy: 0.3847
5760/7185 [=======================>......] - ETA: 0s - loss: 69.2622 - accuracy: 0.3823
6048/7185 [========================>.....] - ETA: 0s - loss: 67.0861 - accuracy: 0.3791
6400/7185 [=========================>....] - ETA: 0s - loss: 64.3467 - accuracy: 0.3772
6720/7185 [===========================>..] - ETA: 0s - loss: 61.4503 - accuracy: 0.3741
7040/7185 [============================>.] - ETA: 0s - loss: 58.8203 - accuracy: 0.3744
7185/7185 [==============================] - 2s 210us/step - loss: 65.7811 - accuracy: 0.3727 - val_loss: 5.8069 - val_accuracy: 0.3701

Umlaut results:
[<Warning: Learning Rate is high>]
Epoch 2/3

  32/7185 [..............................] - ETA: 1s - loss: 33.8203 - accuracy: 0.2500
 352/7185 [>.............................] - ETA: 1s - loss: 31.8362 - accuracy: 0.3892
 672/7185 [=>............................] - ETA: 1s - loss: 55.9012 - accuracy: 0.3363
 992/7185 [===>..........................] - ETA: 0s - loss: 46.5201 - accuracy: 0.3216
1312/7185 [====>.........................] - ETA: 0s - loss: 107.5663 - accuracy: 0.3148
1600/7185 [=====>........................] - ETA: 0s - loss: 104.0556 - accuracy: 0.3250
1888/7185 [======>.......................] - ETA: 0s - loss: 99.7091 - accuracy: 0.3305 
2208/7185 [========>.....................] - ETA: 0s - loss: 85.8939 - accuracy: 0.3311
2560/7185 [=========>....................] - ETA: 0s - loss: 80.8164 - accuracy: 0.3320
2880/7185 [===========>..................] - ETA: 0s - loss: 77.2655 - accuracy: 0.3323
3200/7185 [============>.................] - ETA: 0s - loss: 71.5518 - accuracy: 0.3356
3520/7185 [=============>................] - ETA: 0s - loss: 72.0672 - accuracy: 0.3332
3872/7185 [===============>..............] - ETA: 0s - loss: 74.8643 - accuracy: 0.3352
4224/7185 [================>.............] - ETA: 0s - loss: 69.5697 - accuracy: 0.3326
4544/7185 [=================>............] - ETA: 0s - loss: 65.7604 - accuracy: 0.3391
4864/7185 [===================>..........] - ETA: 0s - loss: 73.1644 - accuracy: 0.3370
5184/7185 [====================>.........] - ETA: 0s - loss: 69.9463 - accuracy: 0.3399
5504/7185 [=====================>........] - ETA: 0s - loss: 67.7443 - accuracy: 0.3347
5824/7185 [=======================>......] - ETA: 0s - loss: 66.8629 - accuracy: 0.3343
6144/7185 [========================>.....] - ETA: 0s - loss: 67.9570 - accuracy: 0.3351
6464/7185 [=========================>....] - ETA: 0s - loss: 64.7462 - accuracy: 0.3334
6784/7185 [===========================>..] - ETA: 0s - loss: 63.1419 - accuracy: 0.3309
7104/7185 [============================>.] - ETA: 0s - loss: 60.9814 - accuracy: 0.3316
7185/7185 [==============================] - 1s 185us/step - loss: 60.3221 - accuracy: 0.3325 - val_loss: 4.1761 - val_accuracy: 0.3589

Umlaut results:
[<Warning: Learning Rate is high>]
Epoch 3/3

  32/7185 [..............................] - ETA: 1s - loss: 2.0019 - accuracy: 0.4375
 352/7185 [>.............................] - ETA: 1s - loss: 7.5219 - accuracy: 0.3608
 704/7185 [=>............................] - ETA: 1s - loss: 5.1032 - accuracy: 0.3423
1056/7185 [===>..........................] - ETA: 0s - loss: 67.7288 - accuracy: 0.3456
1408/7185 [====>.........................] - ETA: 0s - loss: 65.5109 - accuracy: 0.3416
1728/7185 [======>.......................] - ETA: 0s - loss: 56.1335 - accuracy: 0.3252
2016/7185 [=======>......................] - ETA: 0s - loss: 59.3756 - accuracy: 0.3165
2336/7185 [========>.....................] - ETA: 0s - loss: 52.2988 - accuracy: 0.3116
2688/7185 [==========>...................] - ETA: 0s - loss: 45.8597 - accuracy: 0.3211
3008/7185 [===========>..................] - ETA: 0s - loss: 79.0230 - accuracy: 0.3251
3328/7185 [============>.................] - ETA: 0s - loss: 71.7045 - accuracy: 0.3251
3648/7185 [==============>...............] - ETA: 0s - loss: 65.6378 - accuracy: 0.3273
3968/7185 [===============>..............] - ETA: 0s - loss: 61.1804 - accuracy: 0.3347
4288/7185 [================>.............] - ETA: 0s - loss: 57.3107 - accuracy: 0.3386
4608/7185 [==================>...........] - ETA: 0s - loss: 53.7756 - accuracy: 0.3394
4928/7185 [===================>..........] - ETA: 0s - loss: 50.4484 - accuracy: 0.3419
5248/7185 [====================>.........] - ETA: 0s - loss: 47.7069 - accuracy: 0.3405
5568/7185 [======================>.......] - ETA: 0s - loss: 45.1139 - accuracy: 0.3441
5888/7185 [=======================>......] - ETA: 0s - loss: 45.9110 - accuracy: 0.3422
6208/7185 [========================>.....] - ETA: 0s - loss: 51.9197 - accuracy: 0.3431
6528/7185 [==========================>...] - ETA: 0s - loss: 49.9901 - accuracy: 0.3454
6880/7185 [===========================>..] - ETA: 0s - loss: 48.3105 - accuracy: 0.3483
7185/7185 [==============================] - 1s 184us/step - loss: 47.3803 - accuracy: 0.3518 - val_loss: 15.7257 - val_accuracy: 0.3790

Umlaut results:
[<Warning: Learning Rate is high>, <Warning: Possible overfitting>]

  32/2246 [..............................] - ETA: 0s
 544/2246 [======>.......................] - ETA: 0s
1056/2246 [=============>................] - ETA: 0s
1568/2246 [===================>..........] - ETA: 0s
2080/2246 [==========================>...] - ETA: 0s
2246/2246 [==============================] - 0s 101us/step
Test loss: 13.080702009523753
Test accuracy: 0.37845057249069214
D:\nargiz\github\umlaut\venvUMLT\lib\site-packages\keras\datasets\reuters.py:85: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])
D:\nargiz\github\umlaut\venvUMLT\lib\site-packages\keras\datasets\reuters.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])
# of Training Samples: 8982
# of Test Samples: 2246
# of Classes: 46
the an brazil vs reuter 68 vs 2 lt index countries 18 000 reuter all number 000 lt oper site consumption 000 reuter considerably reserves 000 an 784 vs reuter holiday vs some 100 hit our forward up indexes share countries 624 writedown index tonnes income a but reuter countries 624 627 a exchange half be 9 dlrs on bond 07 said in all 100 hit our japanese indexes two as before on tonnes said oper half be in hit our japanese pct dlrs
3
[0. 1. 0. ... 0. 0. 0.]
10000
[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
46
7185
mdl model.h5
['loss', 'accuracy']
Train on 7185 samples, validate on 1797 samples
Umlaut results:
[<Critical: Missing Softmax layer before loss>, <Warning: Last model layer has nonlinear activation>]
Epoch 1/3

  32/7185 [..............................] - ETA: 16s - loss: 3.8440 - accuracy: 0.0312
 288/7185 [>.............................] - ETA: 3s - loss: 1557.1452 - accuracy: 0.3264
 544/7185 [=>............................] - ETA: 2s - loss: 870.6037 - accuracy: 0.3842 
 800/7185 [==>...........................] - ETA: 1s - loss: 617.0112 - accuracy: 0.3738
1088/7185 [===>..........................] - ETA: 1s - loss: 459.9286 - accuracy: 0.3879
1376/7185 [====>.........................] - ETA: 1s - loss: 367.9680 - accuracy: 0.4026
1632/7185 [=====>........................] - ETA: 1s - loss: 316.3906 - accuracy: 0.4069
1920/7185 [=======>......................] - ETA: 1s - loss: 273.4329 - accuracy: 0.4000
2240/7185 [========>.....................] - ETA: 1s - loss: 237.5130 - accuracy: 0.3987
2560/7185 [=========>....................] - ETA: 0s - loss: 209.5252 - accuracy: 0.3934
2880/7185 [===========>..................] - ETA: 0s - loss: 191.4154 - accuracy: 0.3875
3168/7185 [============>.................] - ETA: 0s - loss: 174.4279 - accuracy: 0.3886
3488/7185 [=============>................] - ETA: 0s - loss: 159.4643 - accuracy: 0.3859
3808/7185 [==============>...............] - ETA: 0s - loss: 147.3674 - accuracy: 0.3834
4160/7185 [================>.............] - ETA: 0s - loss: 136.1434 - accuracy: 0.3880
4480/7185 [=================>............] - ETA: 0s - loss: 126.9099 - accuracy: 0.3839
4800/7185 [===================>..........] - ETA: 0s - loss: 120.7316 - accuracy: 0.3821
5120/7185 [====================>.........] - ETA: 0s - loss: 113.3669 - accuracy: 0.3783
5472/7185 [=====================>........] - ETA: 0s - loss: 106.3626 - accuracy: 0.3757
5824/7185 [=======================>......] - ETA: 0s - loss: 100.5399 - accuracy: 0.3740
6144/7185 [========================>.....] - ETA: 0s - loss: 95.6125 - accuracy: 0.3722 
6464/7185 [=========================>....] - ETA: 0s - loss: 92.2554 - accuracy: 0.3730
6784/7185 [===========================>..] - ETA: 0s - loss: 88.6112 - accuracy: 0.3751
7104/7185 [============================>.] - ETA: 0s - loss: 85.0370 - accuracy: 0.3729
7185/7185 [==============================] - 1s 208us/step - loss: 84.1072 - accuracy: 0.3729 - val_loss: 15.9171 - val_accuracy: 0.4073

Umlaut results:
[<Warning: Learning Rate is high>]
Epoch 2/3

  32/7185 [..............................] - ETA: 1s - loss: 7.3635 - accuracy: 0.3750
 352/7185 [>.............................] - ETA: 1s - loss: 15.9294 - accuracy: 0.3920
 672/7185 [=>............................] - ETA: 1s - loss: 24.0771 - accuracy: 0.4122
 992/7185 [===>..........................] - ETA: 0s - loss: 24.6737 - accuracy: 0.3901
1312/7185 [====>.........................] - ETA: 0s - loss: 30.1407 - accuracy: 0.3765
1632/7185 [=====>........................] - ETA: 0s - loss: 31.3911 - accuracy: 0.3854
1952/7185 [=======>......................] - ETA: 0s - loss: 29.5818 - accuracy: 0.3776
2272/7185 [========>.....................] - ETA: 0s - loss: 56.1857 - accuracy: 0.3671
2624/7185 [=========>....................] - ETA: 0s - loss: 62.2427 - accuracy: 0.3613
2944/7185 [===========>..................] - ETA: 0s - loss: 57.3644 - accuracy: 0.3601
3264/7185 [============>.................] - ETA: 0s - loss: 57.1953 - accuracy: 0.3634
3616/7185 [==============>...............] - ETA: 0s - loss: 52.4965 - accuracy: 0.3645
3936/7185 [===============>..............] - ETA: 0s - loss: 49.1362 - accuracy: 0.3590
4256/7185 [================>.............] - ETA: 0s - loss: 46.0283 - accuracy: 0.3640
4576/7185 [==================>...........] - ETA: 0s - loss: 44.4192 - accuracy: 0.3632
4896/7185 [===================>..........] - ETA: 0s - loss: 44.0632 - accuracy: 0.3660
5216/7185 [====================>.........] - ETA: 0s - loss: 44.5609 - accuracy: 0.3715
5536/7185 [======================>.......] - ETA: 0s - loss: 46.1954 - accuracy: 0.3699
5856/7185 [=======================>......] - ETA: 0s - loss: 48.1514 - accuracy: 0.3682
6176/7185 [========================>.....] - ETA: 0s - loss: 47.2034 - accuracy: 0.3674
6464/7185 [=========================>....] - ETA: 0s - loss: 46.9111 - accuracy: 0.3625
6784/7185 [===========================>..] - ETA: 0s - loss: 45.8469 - accuracy: 0.3625
7136/7185 [============================>.] - ETA: 0s - loss: 49.4322 - accuracy: 0.3621
7185/7185 [==============================] - 1s 187us/step - loss: 49.4702 - accuracy: 0.3621 - val_loss: 47.6758 - val_accuracy: 0.3856

Umlaut results:
[<Warning: Learning Rate is high>, <Warning: Possible overfitting>]
Epoch 3/3

  32/7185 [..............................] - ETA: 1s - loss: 3.0715 - accuracy: 0.3750
 352/7185 [>.............................] - ETA: 1s - loss: 117.4264 - accuracy: 0.3608
 672/7185 [=>............................] - ETA: 1s - loss: 92.5364 - accuracy: 0.3720 
 960/7185 [===>..........................] - ETA: 1s - loss: 73.7631 - accuracy: 0.3740
1280/7185 [====>.........................] - ETA: 0s - loss: 61.2084 - accuracy: 0.3688
1568/7185 [=====>........................] - ETA: 0s - loss: 58.8446 - accuracy: 0.3482
1888/7185 [======>.......................] - ETA: 0s - loss: 49.7910 - accuracy: 0.3480
2176/7185 [========>.....................] - ETA: 0s - loss: 46.8801 - accuracy: 0.3488
2496/7185 [=========>....................] - ETA: 0s - loss: 43.6623 - accuracy: 0.3433
2816/7185 [==========>...................] - ETA: 0s - loss: 43.7517 - accuracy: 0.3519
3168/7185 [============>.................] - ETA: 0s - loss: 39.4043 - accuracy: 0.3570
3456/7185 [=============>................] - ETA: 0s - loss: 38.2820 - accuracy: 0.3513
3776/7185 [==============>...............] - ETA: 0s - loss: 36.3334 - accuracy: 0.3496
4096/7185 [================>.............] - ETA: 0s - loss: 47.3620 - accuracy: 0.3423
4416/7185 [=================>............] - ETA: 0s - loss: 65.6394 - accuracy: 0.3440
4704/7185 [==================>...........] - ETA: 0s - loss: 63.2489 - accuracy: 0.3465
5024/7185 [===================>..........] - ETA: 0s - loss: 73.1156 - accuracy: 0.3485
5344/7185 [=====================>........] - ETA: 0s - loss: 71.6131 - accuracy: 0.3466
5664/7185 [======================>.......] - ETA: 0s - loss: 68.4113 - accuracy: 0.3459
5984/7185 [=======================>......] - ETA: 0s - loss: 64.8895 - accuracy: 0.3473
6336/7185 [=========================>....] - ETA: 0s - loss: 65.2376 - accuracy: 0.3477
6656/7185 [==========================>...] - ETA: 0s - loss: 63.0195 - accuracy: 0.3493
6944/7185 [===========================>..] - ETA: 0s - loss: 74.2182 - accuracy: 0.3482
7185/7185 [==============================] - 1s 191us/step - loss: 72.6402 - accuracy: 0.3489 - val_loss: 35.4116 - val_accuracy: 0.3817

Umlaut results:
[<Warning: Learning Rate is high>]

  32/2246 [..............................] - ETA: 0s
 512/2246 [=====>........................] - ETA: 0s
1024/2246 [============>.................] - ETA: 0s
1536/2246 [===================>..........] - ETA: 0s
2048/2246 [==========================>...] - ETA: 0s
2246/2246 [==============================] - 0s 105us/step
Test loss: 45.74814012787327
Test accuracy: 0.37666964530944824
D:\nargiz\github\umlaut\venvUMLT\lib\site-packages\keras\datasets\reuters.py:85: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])
D:\nargiz\github\umlaut\venvUMLT\lib\site-packages\keras\datasets\reuters.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])
# of Training Samples: 8982
# of Test Samples: 2246
# of Classes: 46
the an brazil vs reuter 68 vs 2 lt index countries 18 000 reuter all number 000 lt oper site consumption 000 reuter considerably reserves 000 an 784 vs reuter holiday vs some 100 hit our forward up indexes share countries 624 writedown index tonnes income a but reuter countries 624 627 a exchange half be 9 dlrs on bond 07 said in all 100 hit our japanese indexes two as before on tonnes said oper half be in hit our japanese pct dlrs
3
[0. 1. 0. ... 0. 0. 0.]
10000
[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
46
7185
mdl model.h5
['loss', 'accuracy']
Train on 7185 samples, validate on 1797 samples
Umlaut results:
[<Critical: Missing Softmax layer before loss>, <Warning: Last model layer has nonlinear activation>]
Epoch 1/3

  32/7185 [..............................] - ETA: 16s - loss: 3.9044 - accuracy: 0.0312
 256/7185 [>.............................] - ETA: 3s - loss: 942.4545 - accuracy: 0.3242
 512/7185 [=>............................] - ETA: 2s - loss: 511.1718 - accuracy: 0.3477
 768/7185 [==>...........................] - ETA: 1s - loss: 351.4001 - accuracy: 0.3776
1056/7185 [===>..........................] - ETA: 1s - loss: 263.9587 - accuracy: 0.3598
1312/7185 [====>.........................] - ETA: 1s - loss: 219.5413 - accuracy: 0.3720
1600/7185 [=====>........................] - ETA: 1s - loss: 189.7379 - accuracy: 0.3613
1888/7185 [======>.......................] - ETA: 1s - loss: 171.6592 - accuracy: 0.3591
2208/7185 [========>.....................] - ETA: 1s - loss: 149.5717 - accuracy: 0.3555
2528/7185 [=========>....................] - ETA: 1s - loss: 132.3341 - accuracy: 0.3556
2848/7185 [==========>...................] - ETA: 0s - loss: 118.0764 - accuracy: 0.3469
3136/7185 [============>.................] - ETA: 0s - loss: 107.7597 - accuracy: 0.3450
3488/7185 [=============>................] - ETA: 0s - loss: 101.2570 - accuracy: 0.3483
3808/7185 [==============>...............] - ETA: 0s - loss: 95.6702 - accuracy: 0.3524 
4096/7185 [================>.............] - ETA: 0s - loss: 90.2589 - accuracy: 0.3506
4416/7185 [=================>............] - ETA: 0s - loss: 85.1874 - accuracy: 0.3481
4736/7185 [==================>...........] - ETA: 0s - loss: 79.7793 - accuracy: 0.3480
5056/7185 [====================>.........] - ETA: 0s - loss: 76.7313 - accuracy: 0.3461
5408/7185 [=====================>........] - ETA: 0s - loss: 72.3046 - accuracy: 0.3419
5696/7185 [======================>.......] - ETA: 0s - loss: 69.3646 - accuracy: 0.3408
6016/7185 [========================>.....] - ETA: 0s - loss: 66.0659 - accuracy: 0.3414
6336/7185 [=========================>....] - ETA: 0s - loss: 62.8876 - accuracy: 0.3381
6656/7185 [==========================>...] - ETA: 0s - loss: 60.0786 - accuracy: 0.3394
7008/7185 [============================>.] - ETA: 0s - loss: 57.1859 - accuracy: 0.3403
7185/7185 [==============================] - 2s 211us/step - loss: 55.9456 - accuracy: 0.3385 - val_loss: 4.3375 - val_accuracy: 0.2654

Umlaut results:
[<Warning: Learning Rate is high>]
Epoch 2/3

  32/7185 [..............................] - ETA: 1s - loss: 6.7275 - accuracy: 0.2188
 352/7185 [>.............................] - ETA: 1s - loss: 3.0037 - accuracy: 0.3352
 672/7185 [=>............................] - ETA: 1s - loss: 5.7071 - accuracy: 0.3378
 992/7185 [===>..........................] - ETA: 1s - loss: 5.6318 - accuracy: 0.3609
1312/7185 [====>.........................] - ETA: 0s - loss: 8.8909 - accuracy: 0.3758
1632/7185 [=====>........................] - ETA: 0s - loss: 31.2649 - accuracy: 0.3701
1952/7185 [=======>......................] - ETA: 0s - loss: 64.9283 - accuracy: 0.3689
2240/7185 [========>.....................] - ETA: 0s - loss: 60.7292 - accuracy: 0.3679
2528/7185 [=========>....................] - ETA: 0s - loss: 54.6411 - accuracy: 0.3726
2880/7185 [===========>..................] - ETA: 0s - loss: 50.4064 - accuracy: 0.3726
3232/7185 [============>.................] - ETA: 0s - loss: 59.2406 - accuracy: 0.3719
3584/7185 [=============>................] - ETA: 0s - loss: 53.8053 - accuracy: 0.3730
3904/7185 [===============>..............] - ETA: 0s - loss: 62.5999 - accuracy: 0.3699
4224/7185 [================>.............] - ETA: 0s - loss: 60.3952 - accuracy: 0.3660
4576/7185 [==================>...........] - ETA: 0s - loss: 56.6377 - accuracy: 0.3663
4896/7185 [===================>..........] - ETA: 0s - loss: 55.0652 - accuracy: 0.3674
5216/7185 [====================>.........] - ETA: 0s - loss: 52.2907 - accuracy: 0.3652
5568/7185 [======================>.......] - ETA: 0s - loss: 52.8818 - accuracy: 0.3587
5888/7185 [=======================>......] - ETA: 0s - loss: 58.4254 - accuracy: 0.3568
6208/7185 [========================>.....] - ETA: 0s - loss: 58.3549 - accuracy: 0.3587
6528/7185 [==========================>...] - ETA: 0s - loss: 63.7882 - accuracy: 0.3595
6880/7185 [===========================>..] - ETA: 0s - loss: 69.9863 - accuracy: 0.3562
7185/7185 [==============================] - 1s 188us/step - loss: 67.6163 - accuracy: 0.3543 - val_loss: 3.4254 - val_accuracy: 0.2270

Umlaut results:
[<Warning: Learning Rate is high>]
Epoch 3/3

  32/7185 [..............................] - ETA: 1s - loss: 15.6415 - accuracy: 0.1875
 384/7185 [>.............................] - ETA: 1s - loss: 10.5216 - accuracy: 0.2995
 736/7185 [==>...........................] - ETA: 0s - loss: 11.5229 - accuracy: 0.3342
1056/7185 [===>..........................] - ETA: 0s - loss: 16.4663 - accuracy: 0.3352
1408/7185 [====>.........................] - ETA: 0s - loss: 20.0872 - accuracy: 0.3452
1728/7185 [======>.......................] - ETA: 0s - loss: 21.8325 - accuracy: 0.3438
2016/7185 [=======>......................] - ETA: 0s - loss: 19.1347 - accuracy: 0.3447
2336/7185 [========>.....................] - ETA: 0s - loss: 16.8824 - accuracy: 0.3348
2688/7185 [==========>...................] - ETA: 0s - loss: 24.2284 - accuracy: 0.3408
3008/7185 [===========>..................] - ETA: 0s - loss: 22.2069 - accuracy: 0.3464
3328/7185 [============>.................] - ETA: 0s - loss: 23.1133 - accuracy: 0.3410
3680/7185 [==============>...............] - ETA: 0s - loss: 25.6561 - accuracy: 0.3353
4000/7185 [===============>..............] - ETA: 0s - loss: 26.3082 - accuracy: 0.3327
4320/7185 [=================>............] - ETA: 0s - loss: 31.6743 - accuracy: 0.3354
4640/7185 [==================>...........] - ETA: 0s - loss: 31.2309 - accuracy: 0.3317
4960/7185 [===================>..........] - ETA: 0s - loss: 30.7428 - accuracy: 0.3349
5280/7185 [=====================>........] - ETA: 0s - loss: 29.2117 - accuracy: 0.3392
5600/7185 [======================>.......] - ETA: 0s - loss: 27.6882 - accuracy: 0.3396
5952/7185 [=======================>......] - ETA: 0s - loss: 29.0558 - accuracy: 0.3387
6272/7185 [=========================>....] - ETA: 0s - loss: 29.2788 - accuracy: 0.3418
6592/7185 [==========================>...] - ETA: 0s - loss: 29.6187 - accuracy: 0.3438
6912/7185 [===========================>..] - ETA: 0s - loss: 28.7418 - accuracy: 0.3443
7185/7185 [==============================] - 1s 185us/step - loss: 29.1998 - accuracy: 0.3461 - val_loss: 5.1223 - val_accuracy: 0.3595

Umlaut results:
[<Warning: Learning Rate is high>, <Warning: Possible overfitting>]

  32/2246 [..............................] - ETA: 0s
 576/2246 [======>.......................] - ETA: 0s
1088/2246 [=============>................] - ETA: 0s
1600/2246 [====================>.........] - ETA: 0s
2112/2246 [===========================>..] - ETA: 0s
2246/2246 [==============================] - 0s 99us/step
Test loss: 30.50750395260214
Test accuracy: 0.3584149479866028
D:\nargiz\github\umlaut\venvUMLT\lib\site-packages\keras\datasets\reuters.py:85: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])
D:\nargiz\github\umlaut\venvUMLT\lib\site-packages\keras\datasets\reuters.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])
# of Training Samples: 8982
# of Test Samples: 2246
# of Classes: 46
the an brazil vs reuter 68 vs 2 lt index countries 18 000 reuter all number 000 lt oper site consumption 000 reuter considerably reserves 000 an 784 vs reuter holiday vs some 100 hit our forward up indexes share countries 624 writedown index tonnes income a but reuter countries 624 627 a exchange half be 9 dlrs on bond 07 said in all 100 hit our japanese indexes two as before on tonnes said oper half be in hit our japanese pct dlrs
3
[0. 1. 0. ... 0. 0. 0.]
10000
[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
46
7185
mdl model.h5
['loss', 'accuracy']
Train on 7185 samples, validate on 1797 samples
Umlaut results:
[<Critical: Missing Softmax layer before loss>, <Warning: Last model layer has nonlinear activation>]
Epoch 1/3

  32/7185 [..............................] - ETA: 16s - loss: 3.8538 - accuracy: 0.0312
 288/7185 [>.............................] - ETA: 3s - loss: 738.2419 - accuracy: 0.3403
 544/7185 [=>............................] - ETA: 2s - loss: 428.9014 - accuracy: 0.3732
 800/7185 [==>...........................] - ETA: 1s - loss: 307.1443 - accuracy: 0.4038
1088/7185 [===>..........................] - ETA: 1s - loss: 231.6839 - accuracy: 0.4210
1312/7185 [====>.........................] - ETA: 1s - loss: 198.7709 - accuracy: 0.4245
1568/7185 [=====>........................] - ETA: 1s - loss: 174.8709 - accuracy: 0.4228
1856/7185 [======>.......................] - ETA: 1s - loss: 148.7027 - accuracy: 0.4273
2176/7185 [========>.....................] - ETA: 1s - loss: 152.9060 - accuracy: 0.4200
2496/7185 [=========>....................] - ETA: 1s - loss: 136.2831 - accuracy: 0.4187
2816/7185 [==========>...................] - ETA: 0s - loss: 122.6441 - accuracy: 0.4134
3104/7185 [===========>..................] - ETA: 0s - loss: 113.5737 - accuracy: 0.4191
3424/7185 [=============>................] - ETA: 0s - loss: 104.1930 - accuracy: 0.4103
3744/7185 [==============>...............] - ETA: 0s - loss: 155.4385 - accuracy: 0.4012
4064/7185 [===============>..............] - ETA: 0s - loss: 143.4882 - accuracy: 0.3964
4384/7185 [=================>............] - ETA: 0s - loss: 133.8039 - accuracy: 0.3948
4704/7185 [==================>...........] - ETA: 0s - loss: 125.0816 - accuracy: 0.3926
5024/7185 [===================>..........] - ETA: 0s - loss: 117.7215 - accuracy: 0.3893
5376/7185 [=====================>........] - ETA: 0s - loss: 110.7610 - accuracy: 0.3837
5696/7185 [======================>.......] - ETA: 0s - loss: 104.7527 - accuracy: 0.3811
6016/7185 [========================>.....] - ETA: 0s - loss: 99.3165 - accuracy: 0.3787 
6336/7185 [=========================>....] - ETA: 0s - loss: 96.3660 - accuracy: 0.3744
6688/7185 [==========================>...] - ETA: 0s - loss: 91.5904 - accuracy: 0.3741
7008/7185 [============================>.] - ETA: 0s - loss: 88.5851 - accuracy: 0.3723
7185/7185 [==============================] - 2s 211us/step - loss: 86.5135 - accuracy: 0.3730 - val_loss: 16.6743 - val_accuracy: 0.3701

Umlaut results:
[<Warning: Learning Rate is high>]
Epoch 2/3

  32/7185 [..............................] - ETA: 1s - loss: 34.0344 - accuracy: 0.3438
 384/7185 [>.............................] - ETA: 1s - loss: 45.8842 - accuracy: 0.3438
 704/7185 [=>............................] - ETA: 1s - loss: 27.7814 - accuracy: 0.3665
1024/7185 [===>..........................] - ETA: 0s - loss: 22.4692 - accuracy: 0.3525
1376/7185 [====>.........................] - ETA: 0s - loss: 19.2309 - accuracy: 0.3408
1696/7185 [======>.......................] - ETA: 0s - loss: 20.4693 - accuracy: 0.3426
2016/7185 [=======>......................] - ETA: 0s - loss: 18.3829 - accuracy: 0.3413
2336/7185 [========>.....................] - ETA: 0s - loss: 58.7610 - accuracy: 0.3322
2688/7185 [==========>...................] - ETA: 0s - loss: 58.3164 - accuracy: 0.3352
3008/7185 [===========>..................] - ETA: 0s - loss: 52.8591 - accuracy: 0.3408
3328/7185 [============>.................] - ETA: 0s - loss: 59.5657 - accuracy: 0.3486
3680/7185 [==============>...............] - ETA: 0s - loss: 55.3928 - accuracy: 0.3462
4032/7185 [===============>..............] - ETA: 0s - loss: 53.2447 - accuracy: 0.3450
4352/7185 [=================>............] - ETA: 0s - loss: 51.0979 - accuracy: 0.3506
4672/7185 [==================>...........] - ETA: 0s - loss: 79.3911 - accuracy: 0.3560
4992/7185 [===================>..........] - ETA: 0s - loss: 74.8296 - accuracy: 0.3604
5312/7185 [=====================>........] - ETA: 0s - loss: 117.7458 - accuracy: 0.3598
5632/7185 [======================>.......] - ETA: 0s - loss: 113.4817 - accuracy: 0.3597
5952/7185 [=======================>......] - ETA: 0s - loss: 108.3630 - accuracy: 0.3609
6272/7185 [=========================>....] - ETA: 0s - loss: 105.1952 - accuracy: 0.3586
6592/7185 [==========================>...] - ETA: 0s - loss: 101.5666 - accuracy: 0.3542
6912/7185 [===========================>..] - ETA: 0s - loss: 98.7121 - accuracy: 0.3539 
7185/7185 [==============================] - 1s 186us/step - loss: 95.8255 - accuracy: 0.3546 - val_loss: 7.6928 - val_accuracy: 0.3651

Umlaut results:
[<Warning: Learning Rate is high>]
Epoch 3/3

  32/7185 [..............................] - ETA: 1s - loss: 2.9469 - accuracy: 0.3125
 352/7185 [>.............................] - ETA: 1s - loss: 109.7134 - accuracy: 0.3693
 704/7185 [=>............................] - ETA: 1s - loss: 67.6382 - accuracy: 0.3565 
1056/7185 [===>..........................] - ETA: 0s - loss: 53.6877 - accuracy: 0.3494
1408/7185 [====>.........................] - ETA: 0s - loss: 40.8823 - accuracy: 0.3516
1728/7185 [======>.......................] - ETA: 0s - loss: 39.8832 - accuracy: 0.3495
2016/7185 [=======>......................] - ETA: 0s - loss: 42.7759 - accuracy: 0.3418
2368/7185 [========>.....................] - ETA: 0s - loss: 38.3813 - accuracy: 0.3505
2720/7185 [==========>...................] - ETA: 0s - loss: 35.0861 - accuracy: 0.3526
3040/7185 [===========>..................] - ETA: 0s - loss: 38.9732 - accuracy: 0.3543
3328/7185 [============>.................] - ETA: 0s - loss: 38.1905 - accuracy: 0.3567
3680/7185 [==============>...............] - ETA: 0s - loss: 40.1420 - accuracy: 0.3533
4000/7185 [===============>..............] - ETA: 0s - loss: 103.4448 - accuracy: 0.3550
4320/7185 [=================>............] - ETA: 0s - loss: 98.3406 - accuracy: 0.3525 
4640/7185 [==================>...........] - ETA: 0s - loss: 92.1974 - accuracy: 0.3528
4960/7185 [===================>..........] - ETA: 0s - loss: 86.5767 - accuracy: 0.3488
5280/7185 [=====================>........] - ETA: 0s - loss: 82.7545 - accuracy: 0.3547
5600/7185 [======================>.......] - ETA: 0s - loss: 88.9111 - accuracy: 0.3511
5920/7185 [=======================>......] - ETA: 0s - loss: 101.7439 - accuracy: 0.3507
6240/7185 [=========================>....] - ETA: 0s - loss: 249.8835 - accuracy: 0.3502
6560/7185 [==========================>...] - ETA: 0s - loss: 239.2818 - accuracy: 0.3489
6880/7185 [===========================>..] - ETA: 0s - loss: 228.9571 - accuracy: 0.3493
7185/7185 [==============================] - 1s 186us/step - loss: 220.1422 - accuracy: 0.3464 - val_loss: 5.2404 - val_accuracy: 0.3695

Umlaut results:
[<Warning: Learning Rate is high>]

  32/2246 [..............................] - ETA: 0s
 544/2246 [======>.......................] - ETA: 0s
1056/2246 [=============>................] - ETA: 0s
1536/2246 [===================>..........] - ETA: 0s
2016/2246 [=========================>....] - ETA: 0s
2246/2246 [==============================] - 0s 104us/step
Test loss: 12.365688784888889
Test accuracy: 0.3628673255443573
D:\nargiz\github\umlaut\venvUMLT\lib\site-packages\keras\datasets\reuters.py:85: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])
D:\nargiz\github\umlaut\venvUMLT\lib\site-packages\keras\datasets\reuters.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])
# of Training Samples: 8982
# of Test Samples: 2246
# of Classes: 46
the an brazil vs reuter 68 vs 2 lt index countries 18 000 reuter all number 000 lt oper site consumption 000 reuter considerably reserves 000 an 784 vs reuter holiday vs some 100 hit our forward up indexes share countries 624 writedown index tonnes income a but reuter countries 624 627 a exchange half be 9 dlrs on bond 07 said in all 100 hit our japanese indexes two as before on tonnes said oper half be in hit our japanese pct dlrs
3
[0. 1. 0. ... 0. 0. 0.]
10000
[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
46
7185
mdl model.h5
['loss', 'accuracy']
Train on 7185 samples, validate on 1797 samples
Umlaut results:
[<Critical: Missing Softmax layer before loss>, <Warning: Last model layer has nonlinear activation>]
Epoch 1/3

  32/7185 [..............................] - ETA: 15s - loss: 3.8818 - accuracy: 0.0000e+00
 288/7185 [>.............................] - ETA: 2s - loss: 1164.2889 - accuracy: 0.3056  
 576/7185 [=>............................] - ETA: 2s - loss: 607.1912 - accuracy: 0.2917 
 832/7185 [==>...........................] - ETA: 1s - loss: 431.5475 - accuracy: 0.3293
1088/7185 [===>..........................] - ETA: 1s - loss: 340.9848 - accuracy: 0.3382
1312/7185 [====>.........................] - ETA: 1s - loss: 289.5837 - accuracy: 0.3491
1600/7185 [=====>........................] - ETA: 1s - loss: 241.8100 - accuracy: 0.3512
1888/7185 [======>.......................] - ETA: 1s - loss: 208.6303 - accuracy: 0.3628
2208/7185 [========>.....................] - ETA: 1s - loss: 178.7913 - accuracy: 0.3709
2528/7185 [=========>....................] - ETA: 1s - loss: 158.5151 - accuracy: 0.3758
2816/7185 [==========>...................] - ETA: 0s - loss: 145.5254 - accuracy: 0.3814
3136/7185 [============>.................] - ETA: 0s - loss: 133.6839 - accuracy: 0.3874
3456/7185 [=============>................] - ETA: 0s - loss: 121.9358 - accuracy: 0.3851
3776/7185 [==============>...............] - ETA: 0s - loss: 112.8894 - accuracy: 0.3803
4096/7185 [================>.............] - ETA: 0s - loss: 104.6094 - accuracy: 0.3740
4448/7185 [=================>............] - ETA: 0s - loss: 96.6863 - accuracy: 0.3694 
4768/7185 [==================>...........] - ETA: 0s - loss: 90.3956 - accuracy: 0.3662
5088/7185 [====================>.........] - ETA: 0s - loss: 85.1171 - accuracy: 0.3640
5376/7185 [=====================>........] - ETA: 0s - loss: 80.7903 - accuracy: 0.3666
5728/7185 [======================>.......] - ETA: 0s - loss: 123.7842 - accuracy: 0.3664
6080/7185 [========================>.....] - ETA: 0s - loss: 116.9252 - accuracy: 0.3669
6400/7185 [=========================>....] - ETA: 0s - loss: 111.8600 - accuracy: 0.3659
6720/7185 [===========================>..] - ETA: 0s - loss: 107.1168 - accuracy: 0.3616
7040/7185 [============================>.] - ETA: 0s - loss: 102.6686 - accuracy: 0.3639
7185/7185 [==============================] - 2s 212us/step - loss: 100.7030 - accuracy: 0.3641 - val_loss: 13.6907 - val_accuracy: 0.3723

Umlaut results:
[<Warning: Learning Rate is high>]
Epoch 2/3

  32/7185 [..............................] - ETA: 1s - loss: 14.9835 - accuracy: 0.2812
 352/7185 [>.............................] - ETA: 1s - loss: 156.5217 - accuracy: 0.2955
 704/7185 [=>............................] - ETA: 1s - loss: 104.4196 - accuracy: 0.3182
1056/7185 [===>..........................] - ETA: 0s - loss: 75.1334 - accuracy: 0.3314 
1376/7185 [====>.........................] - ETA: 0s - loss: 60.7761 - accuracy: 0.3285
1696/7185 [======>.......................] - ETA: 0s - loss: 59.9476 - accuracy: 0.3390
1984/7185 [=======>......................] - ETA: 0s - loss: 168.5664 - accuracy: 0.3483
2304/7185 [========>.....................] - ETA: 0s - loss: 148.2264 - accuracy: 0.3529
2592/7185 [=========>....................] - ETA: 0s - loss: 133.7384 - accuracy: 0.3519
2944/7185 [===========>..................] - ETA: 0s - loss: 118.4712 - accuracy: 0.3444
3264/7185 [============>.................] - ETA: 0s - loss: 115.8713 - accuracy: 0.3490
3584/7185 [=============>................] - ETA: 0s - loss: 107.8179 - accuracy: 0.3449
3936/7185 [===============>..............] - ETA: 0s - loss: 103.1689 - accuracy: 0.3432
4256/7185 [================>.............] - ETA: 0s - loss: 98.1573 - accuracy: 0.3430 
4576/7185 [==================>...........] - ETA: 0s - loss: 91.9328 - accuracy: 0.3457
4928/7185 [===================>..........] - ETA: 0s - loss: 86.4434 - accuracy: 0.3474
5280/7185 [=====================>........] - ETA: 0s - loss: 82.4149 - accuracy: 0.3453
5632/7185 [======================>.......] - ETA: 0s - loss: 175.6403 - accuracy: 0.3453
5952/7185 [=======================>......] - ETA: 0s - loss: 172.9350 - accuracy: 0.3449
6240/7185 [=========================>....] - ETA: 0s - loss: 165.6806 - accuracy: 0.3489
6592/7185 [==========================>...] - ETA: 0s - loss: 158.9051 - accuracy: 0.3462
6944/7185 [===========================>..] - ETA: 0s - loss: 151.8105 - accuracy: 0.3476
7185/7185 [==============================] - 1s 187us/step - loss: 147.2369 - accuracy: 0.3439 - val_loss: 82.9714 - val_accuracy: 0.3673

Umlaut results:
[<Warning: Learning Rate is high>]
Epoch 3/3

  32/7185 [..............................] - ETA: 1s - loss: 728.7224 - accuracy: 0.3438
 384/7185 [>.............................] - ETA: 1s - loss: 68.9944 - accuracy: 0.3203 
 672/7185 [=>............................] - ETA: 1s - loss: 53.8680 - accuracy: 0.3824
1024/7185 [===>..........................] - ETA: 1s - loss: 45.5952 - accuracy: 0.3857
1376/7185 [====>.........................] - ETA: 0s - loss: 69.6982 - accuracy: 0.3772
1664/7185 [=====>........................] - ETA: 0s - loss: 71.0617 - accuracy: 0.3726
1984/7185 [=======>......................] - ETA: 0s - loss: 60.2126 - accuracy: 0.3604
2304/7185 [========>.....................] - ETA: 0s - loss: 53.9621 - accuracy: 0.3550
2656/7185 [==========>...................] - ETA: 0s - loss: 56.2444 - accuracy: 0.3566
2976/7185 [===========>..................] - ETA: 0s - loss: 51.1241 - accuracy: 0.3572
3296/7185 [============>.................] - ETA: 0s - loss: 46.8170 - accuracy: 0.3595
3616/7185 [==============>...............] - ETA: 0s - loss: 43.0396 - accuracy: 0.3565
3968/7185 [===============>..............] - ETA: 0s - loss: 43.3944 - accuracy: 0.3516
4288/7185 [================>.............] - ETA: 0s - loss: 64.1760 - accuracy: 0.3517
4640/7185 [==================>...........] - ETA: 0s - loss: 65.0610 - accuracy: 0.3504
4960/7185 [===================>..........] - ETA: 0s - loss: 63.5134 - accuracy: 0.3492
5280/7185 [=====================>........] - ETA: 0s - loss: 61.1170 - accuracy: 0.3481
5600/7185 [======================>.......] - ETA: 0s - loss: 109.7630 - accuracy: 0.3455
5952/7185 [=======================>......] - ETA: 0s - loss: 115.1719 - accuracy: 0.3422
6272/7185 [=========================>....] - ETA: 0s - loss: 110.3436 - accuracy: 0.3404
6592/7185 [==========================>...] - ETA: 0s - loss: 207.7399 - accuracy: 0.3392
6912/7185 [===========================>..] - ETA: 0s - loss: 198.5359 - accuracy: 0.3380
7185/7185 [==============================] - 1s 185us/step - loss: 202.6849 - accuracy: 0.3375 - val_loss: 19.0103 - val_accuracy: 0.3595

Umlaut results:
[<Warning: Learning Rate is high>]

  32/2246 [..............................] - ETA: 0s
 480/2246 [=====>........................] - ETA: 0s
1024/2246 [============>.................] - ETA: 0s
1568/2246 [===================>..........] - ETA: 0s
2048/2246 [==========================>...] - ETA: 0s
2246/2246 [==============================] - 0s 105us/step
Test loss: 45.123542284604596
Test accuracy: 0.35707923769950867
D:\nargiz\github\umlaut\venvUMLT\lib\site-packages\keras\datasets\reuters.py:85: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])
D:\nargiz\github\umlaut\venvUMLT\lib\site-packages\keras\datasets\reuters.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])
# of Training Samples: 8982
# of Test Samples: 2246
# of Classes: 46
the an brazil vs reuter 68 vs 2 lt index countries 18 000 reuter all number 000 lt oper site consumption 000 reuter considerably reserves 000 an 784 vs reuter holiday vs some 100 hit our forward up indexes share countries 624 writedown index tonnes income a but reuter countries 624 627 a exchange half be 9 dlrs on bond 07 said in all 100 hit our japanese indexes two as before on tonnes said oper half be in hit our japanese pct dlrs
3
[0. 1. 0. ... 0. 0. 0.]
10000
[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
46
7185
mdl model.h5
['loss', 'accuracy']
Train on 7185 samples, validate on 1797 samples
Umlaut results:
[<Critical: Missing Softmax layer before loss>, <Warning: Last model layer has nonlinear activation>]
Epoch 1/3

  32/7185 [..............................] - ETA: 16s - loss: 3.8762 - accuracy: 0.0000e+00
 256/7185 [>.............................] - ETA: 3s - loss: 612.4926 - accuracy: 0.2656   
 512/7185 [=>............................] - ETA: 2s - loss: 362.6661 - accuracy: 0.3027
 768/7185 [==>...........................] - ETA: 1s - loss: 258.7528 - accuracy: 0.3503
1024/7185 [===>..........................] - ETA: 1s - loss: 201.1334 - accuracy: 0.3750
1280/7185 [====>.........................] - ETA: 1s - loss: 174.9355 - accuracy: 0.3789
1536/7185 [=====>........................] - ETA: 1s - loss: 154.3705 - accuracy: 0.3867
1792/7185 [======>.......................] - ETA: 1s - loss: 135.8347 - accuracy: 0.3823
2080/7185 [=======>......................] - ETA: 1s - loss: 122.2021 - accuracy: 0.3899
2368/7185 [========>.....................] - ETA: 1s - loss: 110.5878 - accuracy: 0.3915
2688/7185 [==========>...................] - ETA: 1s - loss: 99.4233 - accuracy: 0.3895 
2976/7185 [===========>..................] - ETA: 0s - loss: 93.2515 - accuracy: 0.3898
3296/7185 [============>.................] - ETA: 0s - loss: 86.5327 - accuracy: 0.3841
3616/7185 [==============>...............] - ETA: 0s - loss: 80.8185 - accuracy: 0.3858
3936/7185 [===============>..............] - ETA: 0s - loss: 76.3604 - accuracy: 0.3867
4256/7185 [================>.............] - ETA: 0s - loss: 70.8356 - accuracy: 0.3844
4576/7185 [==================>...........] - ETA: 0s - loss: 66.4494 - accuracy: 0.3824
4896/7185 [===================>..........] - ETA: 0s - loss: 62.5916 - accuracy: 0.3793
5216/7185 [====================>.........] - ETA: 0s - loss: 58.9252 - accuracy: 0.3788
5536/7185 [======================>.......] - ETA: 0s - loss: 56.2954 - accuracy: 0.3750
5856/7185 [=======================>......] - ETA: 0s - loss: 53.6252 - accuracy: 0.3741
6176/7185 [========================>.....] - ETA: 0s - loss: 51.0647 - accuracy: 0.3726
6496/7185 [==========================>...] - ETA: 0s - loss: 52.3982 - accuracy: 0.3715
6848/7185 [===========================>..] - ETA: 0s - loss: 50.0613 - accuracy: 0.3715
7168/7185 [============================>.] - ETA: 0s - loss: 48.0826 - accuracy: 0.3666
7185/7185 [==============================] - 2s 213us/step - loss: 47.9768 - accuracy: 0.3663 - val_loss: 10.6241 - val_accuracy: 0.3595

Umlaut results:
[<Warning: Learning Rate is high>]
Epoch 2/3

  32/7185 [..............................] - ETA: 1s - loss: 2.9047 - accuracy: 0.3125
 352/7185 [>.............................] - ETA: 1s - loss: 11.4855 - accuracy: 0.3182
 640/7185 [=>............................] - ETA: 1s - loss: 12.1788 - accuracy: 0.3187
 960/7185 [===>..........................] - ETA: 1s - loss: 13.6415 - accuracy: 0.3187
1312/7185 [====>.........................] - ETA: 0s - loss: 20.5582 - accuracy: 0.3163
1632/7185 [=====>........................] - ETA: 0s - loss: 18.5627 - accuracy: 0.3174
1952/7185 [=======>......................] - ETA: 0s - loss: 16.0910 - accuracy: 0.3186
2272/7185 [========>.....................] - ETA: 0s - loss: 18.6008 - accuracy: 0.3275
2592/7185 [=========>....................] - ETA: 0s - loss: 17.9650 - accuracy: 0.3275
2912/7185 [===========>..................] - ETA: 0s - loss: 32.8097 - accuracy: 0.3338
3232/7185 [============>.................] - ETA: 0s - loss: 32.6075 - accuracy: 0.3320
3552/7185 [=============>................] - ETA: 0s - loss: 32.5503 - accuracy: 0.3291
3872/7185 [===============>..............] - ETA: 0s - loss: 33.4909 - accuracy: 0.3355
4224/7185 [================>.............] - ETA: 0s - loss: 35.7784 - accuracy: 0.3366
4576/7185 [==================>...........] - ETA: 0s - loss: 37.5375 - accuracy: 0.3416
4896/7185 [===================>..........] - ETA: 0s - loss: 35.8694 - accuracy: 0.3450
5216/7185 [====================>.........] - ETA: 0s - loss: 33.8342 - accuracy: 0.3405
5568/7185 [======================>.......] - ETA: 0s - loss: 32.2438 - accuracy: 0.3409
5920/7185 [=======================>......] - ETA: 0s - loss: 33.8168 - accuracy: 0.3405
6240/7185 [=========================>....] - ETA: 0s - loss: 33.8002 - accuracy: 0.3425
6560/7185 [==========================>...] - ETA: 0s - loss: 33.7149 - accuracy: 0.3404
6848/7185 [===========================>..] - ETA: 0s - loss: 33.2934 - accuracy: 0.3404
7185/7185 [==============================] - 1s 186us/step - loss: 32.0940 - accuracy: 0.3432 - val_loss: 16.2265 - val_accuracy: 0.4508

Umlaut results:
[<Warning: Learning Rate is high>, <Warning: Possible overfitting>]
Epoch 3/3

  32/7185 [..............................] - ETA: 1s - loss: 2.6029 - accuracy: 0.4062
 352/7185 [>.............................] - ETA: 1s - loss: 34.0981 - accuracy: 0.4006
 704/7185 [=>............................] - ETA: 1s - loss: 44.8341 - accuracy: 0.3594
1056/7185 [===>..........................] - ETA: 0s - loss: 31.7531 - accuracy: 0.3589
1408/7185 [====>.........................] - ETA: 0s - loss: 26.6244 - accuracy: 0.3509
1696/7185 [======>.......................] - ETA: 0s - loss: 30.6817 - accuracy: 0.3402
2016/7185 [=======>......................] - ETA: 0s - loss: 46.0710 - accuracy: 0.3343
2336/7185 [========>.....................] - ETA: 0s - loss: 41.6116 - accuracy: 0.3378
2656/7185 [==========>...................] - ETA: 0s - loss: 36.9059 - accuracy: 0.3358
2976/7185 [===========>..................] - ETA: 0s - loss: 38.3996 - accuracy: 0.3280
3296/7185 [============>.................] - ETA: 0s - loss: 35.2322 - accuracy: 0.3301
3616/7185 [==============>...............] - ETA: 0s - loss: 37.1291 - accuracy: 0.3288
3968/7185 [===============>..............] - ETA: 0s - loss: 36.4022 - accuracy: 0.3332
4288/7185 [================>.............] - ETA: 0s - loss: 36.3536 - accuracy: 0.3365
4608/7185 [==================>...........] - ETA: 0s - loss: 37.8300 - accuracy: 0.3407
4928/7185 [===================>..........] - ETA: 0s - loss: 37.5324 - accuracy: 0.3413
5248/7185 [====================>.........] - ETA: 0s - loss: 35.5785 - accuracy: 0.3409
5568/7185 [======================>.......] - ETA: 0s - loss: 35.4696 - accuracy: 0.3385
5888/7185 [=======================>......] - ETA: 0s - loss: 34.1278 - accuracy: 0.3404
6208/7185 [========================>.....] - ETA: 0s - loss: 32.5501 - accuracy: 0.3399
6496/7185 [==========================>...] - ETA: 0s - loss: 33.6239 - accuracy: 0.3410
6816/7185 [===========================>..] - ETA: 0s - loss: 33.0896 - accuracy: 0.3414
7136/7185 [============================>.] - ETA: 0s - loss: 31.9858 - accuracy: 0.3379
7185/7185 [==============================] - 1s 186us/step - loss: 32.0266 - accuracy: 0.3376 - val_loss: 10.3713 - val_accuracy: 0.3600

Umlaut results:
[<Warning: Learning Rate is high>]

  32/2246 [..............................] - ETA: 0s
 576/2246 [======>.......................] - ETA: 0s
1088/2246 [=============>................] - ETA: 0s
1600/2246 [====================>.........] - ETA: 0s
2080/2246 [==========================>...] - ETA: 0s
2246/2246 [==============================] - 0s 101us/step
Test loss: 41.39331456538512
Test accuracy: 0.3499554693698883
D:\nargiz\github\umlaut\venvUMLT\lib\site-packages\keras\datasets\reuters.py:85: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])
D:\nargiz\github\umlaut\venvUMLT\lib\site-packages\keras\datasets\reuters.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])
# of Training Samples: 8982
# of Test Samples: 2246
# of Classes: 46
the an brazil vs reuter 68 vs 2 lt index countries 18 000 reuter all number 000 lt oper site consumption 000 reuter considerably reserves 000 an 784 vs reuter holiday vs some 100 hit our forward up indexes share countries 624 writedown index tonnes income a but reuter countries 624 627 a exchange half be 9 dlrs on bond 07 said in all 100 hit our japanese indexes two as before on tonnes said oper half be in hit our japanese pct dlrs
3
[0. 1. 0. ... 0. 0. 0.]
10000
[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
46
7185
mdl model.h5
['loss', 'accuracy']
Train on 7185 samples, validate on 1797 samples
Umlaut results:
[<Critical: Missing Softmax layer before loss>, <Warning: Last model layer has nonlinear activation>]
Epoch 1/3

  32/7185 [..............................] - ETA: 16s - loss: 3.8776 - accuracy: 0.0000e+00
 288/7185 [>.............................] - ETA: 3s - loss: 1101.0149 - accuracy: 0.3646  
 544/7185 [=>............................] - ETA: 2s - loss: 650.9785 - accuracy: 0.3401 
 800/7185 [==>...........................] - ETA: 1s - loss: 460.7618 - accuracy: 0.3862
1056/7185 [===>..........................] - ETA: 1s - loss: 373.3176 - accuracy: 0.3920
1280/7185 [====>.........................] - ETA: 1s - loss: 314.2257 - accuracy: 0.4000
1568/7185 [=====>........................] - ETA: 1s - loss: 261.7951 - accuracy: 0.4216
1888/7185 [======>.......................] - ETA: 1s - loss: 229.7182 - accuracy: 0.4428
2176/7185 [========>.....................] - ETA: 1s - loss: 206.2705 - accuracy: 0.4458
2464/7185 [=========>....................] - ETA: 1s - loss: 188.3928 - accuracy: 0.4456
2784/7185 [==========>...................] - ETA: 0s - loss: 172.1442 - accuracy: 0.4407
3104/7185 [===========>..................] - ETA: 0s - loss: 157.0207 - accuracy: 0.4336
3392/7185 [=============>................] - ETA: 0s - loss: 144.2596 - accuracy: 0.4281
3712/7185 [==============>...............] - ETA: 0s - loss: 135.2397 - accuracy: 0.4251
4032/7185 [===============>..............] - ETA: 0s - loss: 127.4074 - accuracy: 0.4226
4352/7185 [=================>............] - ETA: 0s - loss: 119.5856 - accuracy: 0.4239
4672/7185 [==================>...........] - ETA: 0s - loss: 113.0612 - accuracy: 0.4193
4992/7185 [===================>..........] - ETA: 0s - loss: 106.4717 - accuracy: 0.4159
5312/7185 [=====================>........] - ETA: 0s - loss: 101.7354 - accuracy: 0.4095
5632/7185 [======================>.......] - ETA: 0s - loss: 97.8124 - accuracy: 0.4057 
5984/7185 [=======================>......] - ETA: 0s - loss: 92.9937 - accuracy: 0.4024
6336/7185 [=========================>....] - ETA: 0s - loss: 89.2091 - accuracy: 0.3969
6656/7185 [==========================>...] - ETA: 0s - loss: 90.3808 - accuracy: 0.3966
6976/7185 [============================>.] - ETA: 0s - loss: 88.6299 - accuracy: 0.3958
7185/7185 [==============================] - 2s 211us/step - loss: 87.3489 - accuracy: 0.3958 - val_loss: 41.1561 - val_accuracy: 0.3801

Umlaut results:
[<Warning: Learning Rate is high>]
Epoch 2/3

  32/7185 [..............................] - ETA: 1s - loss: 26.2202 - accuracy: 0.5625
 384/7185 [>.............................] - ETA: 1s - loss: 40.3119 - accuracy: 0.4271
 736/7185 [==>...........................] - ETA: 1s - loss: 62.2147 - accuracy: 0.3872
1056/7185 [===>..........................] - ETA: 0s - loss: 61.2468 - accuracy: 0.3750
1376/7185 [====>.........................] - ETA: 0s - loss: 55.2235 - accuracy: 0.3663
1664/7185 [=====>........................] - ETA: 0s - loss: 51.1541 - accuracy: 0.3690
1952/7185 [=======>......................] - ETA: 0s - loss: 45.1383 - accuracy: 0.3817
2272/7185 [========>.....................] - ETA: 0s - loss: 42.0859 - accuracy: 0.3864
2592/7185 [=========>....................] - ETA: 0s - loss: 39.7858 - accuracy: 0.3773
2880/7185 [===========>..................] - ETA: 0s - loss: 50.3517 - accuracy: 0.3729
3200/7185 [============>.................] - ETA: 0s - loss: 47.6317 - accuracy: 0.3709
3552/7185 [=============>................] - ETA: 0s - loss: 45.9068 - accuracy: 0.3609
3904/7185 [===============>..............] - ETA: 0s - loss: 48.3728 - accuracy: 0.3637
4256/7185 [================>.............] - ETA: 0s - loss: 51.1735 - accuracy: 0.3640
4576/7185 [==================>...........] - ETA: 0s - loss: 53.2774 - accuracy: 0.3636
4896/7185 [===================>..........] - ETA: 0s - loss: 50.7295 - accuracy: 0.3644
5216/7185 [====================>.........] - ETA: 0s - loss: 52.0540 - accuracy: 0.3639
5536/7185 [======================>.......] - ETA: 0s - loss: 50.3586 - accuracy: 0.3625
5856/7185 [=======================>......] - ETA: 0s - loss: 47.9607 - accuracy: 0.3610
6176/7185 [========================>.....] - ETA: 0s - loss: 95.4271 - accuracy: 0.3593
6496/7185 [==========================>...] - ETA: 0s - loss: 95.6240 - accuracy: 0.3595
6816/7185 [===========================>..] - ETA: 0s - loss: 92.4945 - accuracy: 0.3575
7168/7185 [============================>.] - ETA: 0s - loss: 88.8339 - accuracy: 0.3601
7185/7185 [==============================] - 1s 187us/step - loss: 112.2934 - accuracy: 0.3602 - val_loss: 33.4002 - val_accuracy: 0.4023

Umlaut results:
[<Warning: Learning Rate is high>]
Epoch 3/3

  32/7185 [..............................] - ETA: 1s - loss: 102.9633 - accuracy: 0.2500
 384/7185 [>.............................] - ETA: 1s - loss: 197.0232 - accuracy: 0.3542
 704/7185 [=>............................] - ETA: 1s - loss: 140.1990 - accuracy: 0.3366
1024/7185 [===>..........................] - ETA: 0s - loss: 115.5026 - accuracy: 0.3555
1344/7185 [====>.........................] - ETA: 0s - loss: 105.0207 - accuracy: 0.3579
1664/7185 [=====>........................] - ETA: 0s - loss: 90.9038 - accuracy: 0.3486 
1952/7185 [=======>......................] - ETA: 0s - loss: 78.8345 - accuracy: 0.3381
2272/7185 [========>.....................] - ETA: 0s - loss: 69.1429 - accuracy: 0.3402
2592/7185 [=========>....................] - ETA: 0s - loss: 82.8118 - accuracy: 0.3426
2944/7185 [===========>..................] - ETA: 0s - loss: 74.8594 - accuracy: 0.3485
3264/7185 [============>.................] - ETA: 0s - loss: 84.0462 - accuracy: 0.3459
3584/7185 [=============>................] - ETA: 0s - loss: 84.6043 - accuracy: 0.3432
3936/7185 [===============>..............] - ETA: 0s - loss: 78.5471 - accuracy: 0.3432
4256/7185 [================>.............] - ETA: 0s - loss: 76.3947 - accuracy: 0.3447
4576/7185 [==================>...........] - ETA: 0s - loss: 71.5724 - accuracy: 0.3444
4896/7185 [===================>..........] - ETA: 0s - loss: 67.7093 - accuracy: 0.3380
5216/7185 [====================>.........] - ETA: 0s - loss: 78.2939 - accuracy: 0.3405
5568/7185 [======================>.......] - ETA: 0s - loss: 88.0776 - accuracy: 0.3391
5888/7185 [=======================>......] - ETA: 0s - loss: 89.0707 - accuracy: 0.3415
6208/7185 [========================>.....] - ETA: 0s - loss: 89.5565 - accuracy: 0.3389
6528/7185 [==========================>...] - ETA: 0s - loss: 90.9496 - accuracy: 0.3355
6880/7185 [===========================>..] - ETA: 0s - loss: 87.6087 - accuracy: 0.3376
7185/7185 [==============================] - 1s 185us/step - loss: 84.0504 - accuracy: 0.3356 - val_loss: 108.0291 - val_accuracy: 0.3556

Umlaut results:
[<Warning: Learning Rate is high>, <Warning: Possible overfitting>]

  32/2246 [..............................] - ETA: 0s
 608/2246 [=======>......................] - ETA: 0s
1088/2246 [=============>................] - ETA: 0s
1568/2246 [===================>..........] - ETA: 0s
2080/2246 [==========================>...] - ETA: 0s
2246/2246 [==============================] - 0s 101us/step
Test loss: 92.64315961581305
Test accuracy: 0.35084596276283264
D:\nargiz\github\umlaut\venvUMLT\lib\site-packages\keras\datasets\reuters.py:85: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])
D:\nargiz\github\umlaut\venvUMLT\lib\site-packages\keras\datasets\reuters.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])
# of Training Samples: 8982
# of Test Samples: 2246
# of Classes: 46
the an brazil vs reuter 68 vs 2 lt index countries 18 000 reuter all number 000 lt oper site consumption 000 reuter considerably reserves 000 an 784 vs reuter holiday vs some 100 hit our forward up indexes share countries 624 writedown index tonnes income a but reuter countries 624 627 a exchange half be 9 dlrs on bond 07 said in all 100 hit our japanese indexes two as before on tonnes said oper half be in hit our japanese pct dlrs
3
[0. 1. 0. ... 0. 0. 0.]
10000
[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
46
7185
mdl model.h5
['loss', 'accuracy']
Train on 7185 samples, validate on 1797 samples
Umlaut results:
[<Critical: Missing Softmax layer before loss>, <Warning: Last model layer has nonlinear activation>]
Epoch 1/3

  32/7185 [..............................] - ETA: 15s - loss: 3.8952 - accuracy: 0.0000e+00
 288/7185 [>.............................] - ETA: 3s - loss: 847.3837 - accuracy: 0.3056   
 544/7185 [=>............................] - ETA: 2s - loss: 472.3683 - accuracy: 0.3585
 832/7185 [==>...........................] - ETA: 1s - loss: 330.6750 - accuracy: 0.4038
1120/7185 [===>..........................] - ETA: 1s - loss: 268.0147 - accuracy: 0.4277
1376/7185 [====>.........................] - ETA: 1s - loss: 232.0704 - accuracy: 0.4302
1632/7185 [=====>........................] - ETA: 1s - loss: 205.1345 - accuracy: 0.4203
1920/7185 [=======>......................] - ETA: 1s - loss: 196.2850 - accuracy: 0.4234
2208/7185 [========>.....................] - ETA: 1s - loss: 184.6467 - accuracy: 0.4198
2528/7185 [=========>....................] - ETA: 1s - loss: 164.3426 - accuracy: 0.4225
2848/7185 [==========>...................] - ETA: 0s - loss: 153.5461 - accuracy: 0.4185
3168/7185 [============>.................] - ETA: 0s - loss: 138.7571 - accuracy: 0.4154
3488/7185 [=============>................] - ETA: 0s - loss: 126.3251 - accuracy: 0.4163
3808/7185 [==============>...............] - ETA: 0s - loss: 124.7416 - accuracy: 0.4178
4128/7185 [================>.............] - ETA: 0s - loss: 121.8776 - accuracy: 0.4099
4448/7185 [=================>............] - ETA: 0s - loss: 114.5597 - accuracy: 0.4069
4768/7185 [==================>...........] - ETA: 0s - loss: 107.3114 - accuracy: 0.4052
5120/7185 [====================>.........] - ETA: 0s - loss: 105.8459 - accuracy: 0.4041
5440/7185 [=====================>........] - ETA: 0s - loss: 100.0300 - accuracy: 0.4000
5760/7185 [=======================>......] - ETA: 0s - loss: 95.5550 - accuracy: 0.3957 
6080/7185 [========================>.....] - ETA: 0s - loss: 96.7521 - accuracy: 0.3916
6432/7185 [=========================>....] - ETA: 0s - loss: 92.4589 - accuracy: 0.3905
6784/7185 [===========================>..] - ETA: 0s - loss: 88.1751 - accuracy: 0.3892
7104/7185 [============================>.] - ETA: 0s - loss: 84.8252 - accuracy: 0.3865
7185/7185 [==============================] - 2s 209us/step - loss: 83.8917 - accuracy: 0.3872 - val_loss: 23.1990 - val_accuracy: 0.3545

Umlaut results:
[<Warning: Learning Rate is high>]
Epoch 2/3

  32/7185 [..............................] - ETA: 1s - loss: 2.4275 - accuracy: 0.3750
 352/7185 [>.............................] - ETA: 1s - loss: 42.5398 - accuracy: 0.3381
 640/7185 [=>............................] - ETA: 1s - loss: 29.1780 - accuracy: 0.3688
 960/7185 [===>..........................] - ETA: 1s - loss: 89.0403 - accuracy: 0.3552
1280/7185 [====>.........................] - ETA: 0s - loss: 79.9763 - accuracy: 0.3352
1600/7185 [=====>........................] - ETA: 0s - loss: 66.1783 - accuracy: 0.3425
1920/7185 [=======>......................] - ETA: 0s - loss: 58.9804 - accuracy: 0.3391
2240/7185 [========>.....................] - ETA: 0s - loss: 57.2800 - accuracy: 0.3491
2560/7185 [=========>....................] - ETA: 0s - loss: 53.7992 - accuracy: 0.3582
2880/7185 [===========>..................] - ETA: 0s - loss: 51.9074 - accuracy: 0.3545
3200/7185 [============>.................] - ETA: 0s - loss: 53.8597 - accuracy: 0.3522
3520/7185 [=============>................] - ETA: 0s - loss: 49.4974 - accuracy: 0.3466
3840/7185 [===============>..............] - ETA: 0s - loss: 46.3580 - accuracy: 0.3432
4192/7185 [================>.............] - ETA: 0s - loss: 44.0647 - accuracy: 0.3404
4544/7185 [=================>............] - ETA: 0s - loss: 45.8313 - accuracy: 0.3435
4864/7185 [===================>..........] - ETA: 0s - loss: 44.3434 - accuracy: 0.3415
5184/7185 [====================>.........] - ETA: 0s - loss: 44.6026 - accuracy: 0.3438
5504/7185 [=====================>........] - ETA: 0s - loss: 43.1409 - accuracy: 0.3403
5824/7185 [=======================>......] - ETA: 0s - loss: 45.7805 - accuracy: 0.3405
6144/7185 [========================>.....] - ETA: 0s - loss: 72.2591 - accuracy: 0.3397
6464/7185 [=========================>....] - ETA: 0s - loss: 69.5586 - accuracy: 0.3410
6784/7185 [===========================>..] - ETA: 0s - loss: 70.2159 - accuracy: 0.3423
7104/7185 [============================>.] - ETA: 0s - loss: 69.9049 - accuracy: 0.3407
7185/7185 [==============================] - 1s 186us/step - loss: 69.1461 - accuracy: 0.3406 - val_loss: 75.9017 - val_accuracy: 0.3534

Umlaut results:
[<Warning: Learning Rate is high>, <Warning: Possible overfitting>]
Epoch 3/3

  32/7185 [..............................] - ETA: 1s - loss: 7.9952 - accuracy: 0.3438
 352/7185 [>.............................] - ETA: 1s - loss: 22.4632 - accuracy: 0.2756
 704/7185 [=>............................] - ETA: 1s - loss: 38.3374 - accuracy: 0.3295
1024/7185 [===>..........................] - ETA: 0s - loss: 42.2313 - accuracy: 0.3145
1344/7185 [====>.........................] - ETA: 0s - loss: 36.4193 - accuracy: 0.3289
1664/7185 [=====>........................] - ETA: 0s - loss: 33.8974 - accuracy: 0.3281
1952/7185 [=======>......................] - ETA: 0s - loss: 89.9099 - accuracy: 0.3304
2272/7185 [========>.....................] - ETA: 0s - loss: 109.4258 - accuracy: 0.3393
2592/7185 [=========>....................] - ETA: 0s - loss: 99.5282 - accuracy: 0.3360 
2944/7185 [===========>..................] - ETA: 0s - loss: 206.1873 - accuracy: 0.3305
3296/7185 [============>.................] - ETA: 0s - loss: 188.1656 - accuracy: 0.3313
3616/7185 [==============>...............] - ETA: 0s - loss: 174.0582 - accuracy: 0.3319
3936/7185 [===============>..............] - ETA: 0s - loss: 160.1150 - accuracy: 0.3305
4288/7185 [================>.............] - ETA: 0s - loss: 180.3620 - accuracy: 0.3281
4608/7185 [==================>...........] - ETA: 0s - loss: 178.1617 - accuracy: 0.3303
4928/7185 [===================>..........] - ETA: 0s - loss: 167.2177 - accuracy: 0.3302
5248/7185 [====================>.........] - ETA: 0s - loss: 159.8049 - accuracy: 0.3304
5568/7185 [======================>.......] - ETA: 0s - loss: 201.2758 - accuracy: 0.3346
5888/7185 [=======================>......] - ETA: 0s - loss: 191.0665 - accuracy: 0.3370
6208/7185 [========================>.....] - ETA: 0s - loss: 183.0063 - accuracy: 0.3380
6528/7185 [==========================>...] - ETA: 0s - loss: 174.5161 - accuracy: 0.3378
6848/7185 [===========================>..] - ETA: 0s - loss: 169.9083 - accuracy: 0.3389
7168/7185 [============================>.] - ETA: 0s - loss: 166.4512 - accuracy: 0.3426
7185/7185 [==============================] - 1s 185us/step - loss: 166.0825 - accuracy: 0.3421 - val_loss: 54.0488 - val_accuracy: 0.2142

Umlaut results:
[<Warning: Learning Rate is high>]

  32/2246 [..............................] - ETA: 0s
 608/2246 [=======>......................] - ETA: 0s
1152/2246 [==============>...............] - ETA: 0s
1696/2246 [=====================>........] - ETA: 0s
2208/2246 [============================>.] - ETA: 0s
2246/2246 [==============================] - 0s 95us/step
Test loss: 25.282144512327676
Test accuracy: 0.22796082496643066
D:\nargiz\github\umlaut\venvUMLT\lib\site-packages\keras\datasets\reuters.py:85: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])
D:\nargiz\github\umlaut\venvUMLT\lib\site-packages\keras\datasets\reuters.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])
# of Training Samples: 8982
# of Test Samples: 2246
# of Classes: 46
the an brazil vs reuter 68 vs 2 lt index countries 18 000 reuter all number 000 lt oper site consumption 000 reuter considerably reserves 000 an 784 vs reuter holiday vs some 100 hit our forward up indexes share countries 624 writedown index tonnes income a but reuter countries 624 627 a exchange half be 9 dlrs on bond 07 said in all 100 hit our japanese indexes two as before on tonnes said oper half be in hit our japanese pct dlrs
3
[0. 1. 0. ... 0. 0. 0.]
10000
[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
46
7185
mdl model.h5
['loss', 'accuracy']
Train on 7185 samples, validate on 1797 samples
Umlaut results:
[<Critical: Missing Softmax layer before loss>, <Warning: Last model layer has nonlinear activation>]
Epoch 1/3

  32/7185 [..............................] - ETA: 16s - loss: 3.8708 - accuracy: 0.0312
 320/7185 [>.............................] - ETA: 2s - loss: 660.7574 - accuracy: 0.3531
 576/7185 [=>............................] - ETA: 2s - loss: 380.9974 - accuracy: 0.3802
 832/7185 [==>...........................] - ETA: 1s - loss: 272.1279 - accuracy: 0.3858
1088/7185 [===>..........................] - ETA: 1s - loss: 216.2746 - accuracy: 0.4099
1344/7185 [====>.........................] - ETA: 1s - loss: 186.1618 - accuracy: 0.4167
1632/7185 [=====>........................] - ETA: 1s - loss: 160.5957 - accuracy: 0.4252
1952/7185 [=======>......................] - ETA: 1s - loss: 138.9216 - accuracy: 0.4150
2272/7185 [========>.....................] - ETA: 1s - loss: 121.5459 - accuracy: 0.4181
2592/7185 [=========>....................] - ETA: 0s - loss: 110.7217 - accuracy: 0.4151
2912/7185 [===========>..................] - ETA: 0s - loss: 100.1700 - accuracy: 0.4141
3232/7185 [============>.................] - ETA: 0s - loss: 92.7834 - accuracy: 0.4152 
3552/7185 [=============>................] - ETA: 0s - loss: 86.9986 - accuracy: 0.4212
3872/7185 [===============>..............] - ETA: 0s - loss: 81.9630 - accuracy: 0.4228
4192/7185 [================>.............] - ETA: 0s - loss: 76.0351 - accuracy: 0.4182
4544/7185 [=================>............] - ETA: 0s - loss: 71.6687 - accuracy: 0.4124
4896/7185 [===================>..........] - ETA: 0s - loss: 67.0297 - accuracy: 0.4062
5184/7185 [====================>.........] - ETA: 0s - loss: 63.5117 - accuracy: 0.4039
5504/7185 [=====================>........] - ETA: 0s - loss: 61.0264 - accuracy: 0.4006
5760/7185 [=======================>......] - ETA: 0s - loss: 58.4829 - accuracy: 0.3998
6080/7185 [========================>.....] - ETA: 0s - loss: 56.5510 - accuracy: 0.3977
6400/7185 [=========================>....] - ETA: 0s - loss: 54.2939 - accuracy: 0.3925
6720/7185 [===========================>..] - ETA: 0s - loss: 53.2197 - accuracy: 0.3866
7072/7185 [============================>.] - ETA: 0s - loss: 51.1082 - accuracy: 0.3856
7185/7185 [==============================] - 1s 208us/step - loss: 50.3538 - accuracy: 0.3855 - val_loss: 90.8614 - val_accuracy: 0.4240

Umlaut results:
[<Warning: Learning Rate is high>]
Epoch 2/3

  32/7185 [..............................] - ETA: 1s - loss: 2.4141 - accuracy: 0.3438
 320/7185 [>.............................] - ETA: 1s - loss: 14.2662 - accuracy: 0.4031
 640/7185 [=>............................] - ETA: 1s - loss: 45.5390 - accuracy: 0.3578
 992/7185 [===>..........................] - ETA: 1s - loss: 46.3185 - accuracy: 0.3609
1312/7185 [====>.........................] - ETA: 0s - loss: 42.6712 - accuracy: 0.3407
1600/7185 [=====>........................] - ETA: 0s - loss: 36.4403 - accuracy: 0.3456
1920/7185 [=======>......................] - ETA: 0s - loss: 30.9496 - accuracy: 0.3438
2208/7185 [========>.....................] - ETA: 0s - loss: 29.8483 - accuracy: 0.3451
2528/7185 [=========>....................] - ETA: 0s - loss: 28.9509 - accuracy: 0.3493
2848/7185 [==========>...................] - ETA: 0s - loss: 28.6645 - accuracy: 0.3511
3168/7185 [============>.................] - ETA: 0s - loss: 28.2560 - accuracy: 0.3488
3488/7185 [=============>................] - ETA: 0s - loss: 29.3172 - accuracy: 0.3489
3808/7185 [==============>...............] - ETA: 0s - loss: 27.0646 - accuracy: 0.3506
4128/7185 [================>.............] - ETA: 0s - loss: 25.8079 - accuracy: 0.3534
4480/7185 [=================>............] - ETA: 0s - loss: 24.5343 - accuracy: 0.3556
4832/7185 [===================>..........] - ETA: 0s - loss: 24.9710 - accuracy: 0.3514
5152/7185 [====================>.........] - ETA: 0s - loss: 25.9199 - accuracy: 0.3484
5504/7185 [=====================>........] - ETA: 0s - loss: 26.4581 - accuracy: 0.3514
5824/7185 [=======================>......] - ETA: 0s - loss: 28.8220 - accuracy: 0.3511
6144/7185 [========================>.....] - ETA: 0s - loss: 29.8852 - accuracy: 0.3538
6464/7185 [=========================>....] - ETA: 0s - loss: 28.5543 - accuracy: 0.3544
6816/7185 [===========================>..] - ETA: 0s - loss: 28.0932 - accuracy: 0.3556
7136/7185 [============================>.] - ETA: 0s - loss: 28.3751 - accuracy: 0.3568
7185/7185 [==============================] - 1s 189us/step - loss: 28.1997 - accuracy: 0.3559 - val_loss: 20.5539 - val_accuracy: 0.2142

Umlaut results:
[<Warning: Learning Rate is high>]
Epoch 3/3

  32/7185 [..............................] - ETA: 1s - loss: 2.7639 - accuracy: 0.1250
 320/7185 [>.............................] - ETA: 1s - loss: 7.2770 - accuracy: 0.3344
 640/7185 [=>............................] - ETA: 1s - loss: 71.4584 - accuracy: 0.3719
 992/7185 [===>..........................] - ETA: 1s - loss: 107.0437 - accuracy: 0.3518
1312/7185 [====>.........................] - ETA: 0s - loss: 98.7916 - accuracy: 0.3651 
1600/7185 [=====>........................] - ETA: 0s - loss: 106.3164 - accuracy: 0.3525
1856/7185 [======>.......................] - ETA: 0s - loss: 99.4643 - accuracy: 0.3599 
2208/7185 [========>.....................] - ETA: 0s - loss: 106.9351 - accuracy: 0.3451
2560/7185 [=========>....................] - ETA: 0s - loss: 99.8934 - accuracy: 0.3480 
2880/7185 [===========>..................] - ETA: 0s - loss: 105.4124 - accuracy: 0.3490
3200/7185 [============>.................] - ETA: 0s - loss: 96.7187 - accuracy: 0.3444 
3520/7185 [=============>................] - ETA: 0s - loss: 93.4627 - accuracy: 0.3389
3872/7185 [===============>..............] - ETA: 0s - loss: 85.7694 - accuracy: 0.3386
4224/7185 [================>.............] - ETA: 0s - loss: 83.2310 - accuracy: 0.3435
4544/7185 [=================>............] - ETA: 0s - loss: 80.2598 - accuracy: 0.3444
4864/7185 [===================>..........] - ETA: 0s - loss: 75.3691 - accuracy: 0.3460
5152/7185 [====================>.........] - ETA: 0s - loss: 76.1374 - accuracy: 0.3463
5440/7185 [=====================>........] - ETA: 0s - loss: 73.2271 - accuracy: 0.3474
5760/7185 [=======================>......] - ETA: 0s - loss: 69.9160 - accuracy: 0.3458
6080/7185 [========================>.....] - ETA: 0s - loss: 66.3713 - accuracy: 0.3438
6400/7185 [=========================>....] - ETA: 0s - loss: 76.4867 - accuracy: 0.3406
6720/7185 [===========================>..] - ETA: 0s - loss: 76.7247 - accuracy: 0.3403
7040/7185 [============================>.] - ETA: 0s - loss: 77.1709 - accuracy: 0.3406
7185/7185 [==============================] - 1s 189us/step - loss: 105.1112 - accuracy: 0.3395 - val_loss: 33.1326 - val_accuracy: 0.3723

Umlaut results:
[<Warning: Learning Rate is high>]

  32/2246 [..............................] - ETA: 0s
 608/2246 [=======>......................] - ETA: 0s
1184/2246 [==============>...............] - ETA: 0s
1696/2246 [=====================>........] - ETA: 0s
2246/2246 [==============================] - 0s 93us/step
Test loss: 40.92603351447801
Test accuracy: 0.36553874611854553
D:\nargiz\github\umlaut\venvUMLT\lib\site-packages\keras\datasets\reuters.py:85: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])
D:\nargiz\github\umlaut\venvUMLT\lib\site-packages\keras\datasets\reuters.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])
# of Training Samples: 8982
# of Test Samples: 2246
# of Classes: 46
the an brazil vs reuter 68 vs 2 lt index countries 18 000 reuter all number 000 lt oper site consumption 000 reuter considerably reserves 000 an 784 vs reuter holiday vs some 100 hit our forward up indexes share countries 624 writedown index tonnes income a but reuter countries 624 627 a exchange half be 9 dlrs on bond 07 said in all 100 hit our japanese indexes two as before on tonnes said oper half be in hit our japanese pct dlrs
3
[0. 1. 0. ... 0. 0. 0.]
10000
[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
46
7185
mdl model.h5
['loss', 'accuracy']
Train on 7185 samples, validate on 1797 samples
Umlaut results:
[<Critical: Missing Softmax layer before loss>, <Warning: Last model layer has nonlinear activation>]
Epoch 1/3

  32/7185 [..............................] - ETA: 16s - loss: 3.8383 - accuracy: 0.0312
 288/7185 [>.............................] - ETA: 3s - loss: 534.6396 - accuracy: 0.2986
 544/7185 [=>............................] - ETA: 2s - loss: 310.2963 - accuracy: 0.3585
 800/7185 [==>...........................] - ETA: 1s - loss: 235.1342 - accuracy: 0.3825
1056/7185 [===>..........................] - ETA: 1s - loss: 192.3773 - accuracy: 0.4034
1344/7185 [====>.........................] - ETA: 1s - loss: 153.7006 - accuracy: 0.4196
1600/7185 [=====>........................] - ETA: 1s - loss: 143.3151 - accuracy: 0.4288
1888/7185 [======>.......................] - ETA: 1s - loss: 126.9926 - accuracy: 0.4311
2176/7185 [========>.....................] - ETA: 1s - loss: 112.5763 - accuracy: 0.4315
2464/7185 [=========>....................] - ETA: 1s - loss: 101.4213 - accuracy: 0.4265
2784/7185 [==========>...................] - ETA: 0s - loss: 90.6469 - accuracy: 0.4159 
3104/7185 [===========>..................] - ETA: 0s - loss: 83.1327 - accuracy: 0.4095
3424/7185 [=============>................] - ETA: 0s - loss: 76.4551 - accuracy: 0.4054
3744/7185 [==============>...............] - ETA: 0s - loss: 72.2278 - accuracy: 0.3966
4064/7185 [===============>..............] - ETA: 0s - loss: 66.7586 - accuracy: 0.3932
4384/7185 [=================>............] - ETA: 0s - loss: 62.5423 - accuracy: 0.3942
4704/7185 [==================>...........] - ETA: 0s - loss: 58.5374 - accuracy: 0.3950
5056/7185 [====================>.........] - ETA: 0s - loss: 55.4684 - accuracy: 0.3902
5376/7185 [=====================>........] - ETA: 0s - loss: 54.2982 - accuracy: 0.3854
5696/7185 [======================>.......] - ETA: 0s - loss: 51.8432 - accuracy: 0.3813
6016/7185 [========================>.....] - ETA: 0s - loss: 49.3268 - accuracy: 0.3783
6336/7185 [=========================>....] - ETA: 0s - loss: 46.9775 - accuracy: 0.3753
6656/7185 [==========================>...] - ETA: 0s - loss: 44.9438 - accuracy: 0.3774
6944/7185 [===========================>..] - ETA: 0s - loss: 43.2752 - accuracy: 0.3760
7185/7185 [==============================] - 2s 211us/step - loss: 42.0065 - accuracy: 0.3717 - val_loss: 13.9563 - val_accuracy: 0.2554

Umlaut results:
[<Warning: Learning Rate is high>]
Epoch 2/3

  32/7185 [..............................] - ETA: 1s - loss: 21.1704 - accuracy: 0.2188
 352/7185 [>.............................] - ETA: 1s - loss: 11.1454 - accuracy: 0.2926
 704/7185 [=>............................] - ETA: 1s - loss: 30.0637 - accuracy: 0.3097
1024/7185 [===>..........................] - ETA: 0s - loss: 36.9620 - accuracy: 0.3174
1376/7185 [====>.........................] - ETA: 0s - loss: 37.7781 - accuracy: 0.3219
1664/7185 [=====>........................] - ETA: 0s - loss: 34.1691 - accuracy: 0.3233
1952/7185 [=======>......................] - ETA: 0s - loss: 32.8398 - accuracy: 0.3186
2272/7185 [========>.....................] - ETA: 0s - loss: 30.2632 - accuracy: 0.3235
2624/7185 [=========>....................] - ETA: 0s - loss: 27.3364 - accuracy: 0.3209
2944/7185 [===========>..................] - ETA: 0s - loss: 28.6083 - accuracy: 0.3200
3264/7185 [============>.................] - ETA: 0s - loss: 35.9139 - accuracy: 0.3229
3584/7185 [=============>................] - ETA: 0s - loss: 46.6849 - accuracy: 0.3284
3904/7185 [===============>..............] - ETA: 0s - loss: 44.4432 - accuracy: 0.3356
4224/7185 [================>.............] - ETA: 0s - loss: 42.8922 - accuracy: 0.3402
4544/7185 [=================>............] - ETA: 0s - loss: 40.7327 - accuracy: 0.3396
4864/7185 [===================>..........] - ETA: 0s - loss: 41.8297 - accuracy: 0.3378
5184/7185 [====================>.........] - ETA: 0s - loss: 40.8265 - accuracy: 0.3383
5504/7185 [=====================>........] - ETA: 0s - loss: 39.6981 - accuracy: 0.3403
5856/7185 [=======================>......] - ETA: 0s - loss: 37.6763 - accuracy: 0.3422
6208/7185 [========================>.....] - ETA: 0s - loss: 36.9772 - accuracy: 0.3450
6496/7185 [==========================>...] - ETA: 0s - loss: 35.6804 - accuracy: 0.3444
6816/7185 [===========================>..] - ETA: 0s - loss: 34.4537 - accuracy: 0.3418
7136/7185 [============================>.] - ETA: 0s - loss: 33.3133 - accuracy: 0.3409
7185/7185 [==============================] - 1s 187us/step - loss: 35.2429 - accuracy: 0.3417 - val_loss: 72.5067 - val_accuracy: 0.3795

Umlaut results:
[<Warning: Learning Rate is high>, <Warning: Possible overfitting>]
Epoch 3/3

  32/7185 [..............................] - ETA: 1s - loss: 539.0732 - accuracy: 0.2188
 352/7185 [>.............................] - ETA: 1s - loss: 83.5649 - accuracy: 0.3409 
 672/7185 [=>............................] - ETA: 1s - loss: 50.8381 - accuracy: 0.3289
1024/7185 [===>..........................] - ETA: 0s - loss: 36.8994 - accuracy: 0.3281
1344/7185 [====>.........................] - ETA: 0s - loss: 71.3645 - accuracy: 0.3311
1664/7185 [=====>........................] - ETA: 0s - loss: 60.6089 - accuracy: 0.3179
1952/7185 [=======>......................] - ETA: 0s - loss: 52.4788 - accuracy: 0.3238
2272/7185 [========>.....................] - ETA: 0s - loss: 45.8128 - accuracy: 0.3178
2592/7185 [=========>....................] - ETA: 0s - loss: 47.7330 - accuracy: 0.3256
2912/7185 [===========>..................] - ETA: 0s - loss: 54.0727 - accuracy: 0.3304
3232/7185 [============>.................] - ETA: 0s - loss: 50.0103 - accuracy: 0.3308
3552/7185 [=============>................] - ETA: 0s - loss: 45.7573 - accuracy: 0.3288
3872/7185 [===============>..............] - ETA: 0s - loss: 42.5880 - accuracy: 0.3290
4224/7185 [================>.............] - ETA: 0s - loss: 39.2564 - accuracy: 0.3319
4544/7185 [=================>............] - ETA: 0s - loss: 36.8343 - accuracy: 0.3332
4864/7185 [===================>..........] - ETA: 0s - loss: 35.4363 - accuracy: 0.3324
5216/7185 [====================>.........] - ETA: 0s - loss: 36.0090 - accuracy: 0.3309
5536/7185 [======================>.......] - ETA: 0s - loss: 35.8412 - accuracy: 0.3338
5856/7185 [=======================>......] - ETA: 0s - loss: 36.2377 - accuracy: 0.3321
6176/7185 [========================>.....] - ETA: 0s - loss: 34.4894 - accuracy: 0.3348
6496/7185 [==========================>...] - ETA: 0s - loss: 32.9793 - accuracy: 0.3350
6816/7185 [===========================>..] - ETA: 0s - loss: 32.0410 - accuracy: 0.3335
7136/7185 [============================>.] - ETA: 0s - loss: 30.7267 - accuracy: 0.3320
7185/7185 [==============================] - 1s 187us/step - loss: 30.5400 - accuracy: 0.3322 - val_loss: 49.1805 - val_accuracy: 0.3556

Umlaut results:
[<Warning: Learning Rate is high>]

  32/2246 [..............................] - ETA: 0s
 512/2246 [=====>........................] - ETA: 0s
1056/2246 [=============>................] - ETA: 0s
1600/2246 [====================>.........] - ETA: 0s
2144/2246 [===========================>..] - ETA: 0s
2246/2246 [==============================] - 0s 98us/step
Test loss: 4.0066568809009935
Test accuracy: 0.3535173535346985
D:\nargiz\github\umlaut\venvUMLT\lib\site-packages\keras\datasets\reuters.py:85: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])
D:\nargiz\github\umlaut\venvUMLT\lib\site-packages\keras\datasets\reuters.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])
# of Training Samples: 8982
# of Test Samples: 2246
# of Classes: 46
the an brazil vs reuter 68 vs 2 lt index countries 18 000 reuter all number 000 lt oper site consumption 000 reuter considerably reserves 000 an 784 vs reuter holiday vs some 100 hit our forward up indexes share countries 624 writedown index tonnes income a but reuter countries 624 627 a exchange half be 9 dlrs on bond 07 said in all 100 hit our japanese indexes two as before on tonnes said oper half be in hit our japanese pct dlrs
3
[0. 1. 0. ... 0. 0. 0.]
10000
[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
46
7185
mdl model.h5
['loss', 'accuracy']
Train on 7185 samples, validate on 1797 samples
Umlaut results:
[<Critical: Missing Softmax layer before loss>, <Warning: Last model layer has nonlinear activation>]
Epoch 1/3

  32/7185 [..............................] - ETA: 15s - loss: 3.8480 - accuracy: 0.0312
 288/7185 [>.............................] - ETA: 2s - loss: 1109.4543 - accuracy: 0.3229
 576/7185 [=>............................] - ETA: 2s - loss: 614.8549 - accuracy: 0.3403 
 832/7185 [==>...........................] - ETA: 1s - loss: 437.4696 - accuracy: 0.3654
1088/7185 [===>..........................] - ETA: 1s - loss: 357.6841 - accuracy: 0.3796
1344/7185 [====>.........................] - ETA: 1s - loss: 293.0979 - accuracy: 0.3839
1600/7185 [=====>........................] - ETA: 1s - loss: 247.0661 - accuracy: 0.3963
1888/7185 [======>.......................] - ETA: 1s - loss: 211.8302 - accuracy: 0.4062
2208/7185 [========>.....................] - ETA: 1s - loss: 190.2089 - accuracy: 0.3927
2528/7185 [=========>....................] - ETA: 1s - loss: 169.7073 - accuracy: 0.3896
2816/7185 [==========>...................] - ETA: 0s - loss: 156.1652 - accuracy: 0.3960
3136/7185 [============>.................] - ETA: 0s - loss: 143.2785 - accuracy: 0.3954
3456/7185 [=============>................] - ETA: 0s - loss: 135.0707 - accuracy: 0.3898
3744/7185 [==============>...............] - ETA: 0s - loss: 125.2854 - accuracy: 0.3851
4064/7185 [===============>..............] - ETA: 0s - loss: 116.9893 - accuracy: 0.3787
4352/7185 [=================>............] - ETA: 0s - loss: 111.2561 - accuracy: 0.3752
4672/7185 [==================>...........] - ETA: 0s - loss: 105.6768 - accuracy: 0.3756
4992/7185 [===================>..........] - ETA: 0s - loss: 99.3887 - accuracy: 0.3748 
5280/7185 [=====================>........] - ETA: 0s - loss: 94.6287 - accuracy: 0.3746
5600/7185 [======================>.......] - ETA: 0s - loss: 90.1081 - accuracy: 0.3688
5920/7185 [=======================>......] - ETA: 0s - loss: 86.2025 - accuracy: 0.3677
6240/7185 [=========================>....] - ETA: 0s - loss: 83.2381 - accuracy: 0.3668
6560/7185 [==========================>...] - ETA: 0s - loss: 82.0145 - accuracy: 0.3671
6880/7185 [===========================>..] - ETA: 0s - loss: 78.4822 - accuracy: 0.3670
7185/7185 [==============================] - 2s 213us/step - loss: 76.5600 - accuracy: 0.3653 - val_loss: 13.8331 - val_accuracy: 0.3205

Umlaut results:
[<Warning: Learning Rate is high>]
Epoch 2/3

  32/7185 [..............................] - ETA: 1s - loss: 4.1754 - accuracy: 0.3750
 352/7185 [>.............................] - ETA: 1s - loss: 6.2990 - accuracy: 0.3352
 672/7185 [=>............................] - ETA: 1s - loss: 27.1752 - accuracy: 0.3408
 960/7185 [===>..........................] - ETA: 1s - loss: 39.4395 - accuracy: 0.3281
1280/7185 [====>.........................] - ETA: 0s - loss: 34.2934 - accuracy: 0.3141
1568/7185 [=====>........................] - ETA: 0s - loss: 34.4462 - accuracy: 0.3233
1856/7185 [======>.......................] - ETA: 0s - loss: 29.9958 - accuracy: 0.3335
2176/7185 [========>.....................] - ETA: 0s - loss: 27.8783 - accuracy: 0.3369
2496/7185 [=========>....................] - ETA: 0s - loss: 28.9604 - accuracy: 0.3405
2816/7185 [==========>...................] - ETA: 0s - loss: 31.1885 - accuracy: 0.3324
3136/7185 [============>.................] - ETA: 0s - loss: 34.6590 - accuracy: 0.3358
3456/7185 [=============>................] - ETA: 0s - loss: 34.2226 - accuracy: 0.3374
3776/7185 [==============>...............] - ETA: 0s - loss: 32.6747 - accuracy: 0.3358
4096/7185 [================>.............] - ETA: 0s - loss: 51.7862 - accuracy: 0.3340
4416/7185 [=================>............] - ETA: 0s - loss: 48.3971 - accuracy: 0.3317
4736/7185 [==================>...........] - ETA: 0s - loss: 46.0771 - accuracy: 0.3351
5056/7185 [====================>.........] - ETA: 0s - loss: 44.6150 - accuracy: 0.3362
5376/7185 [=====================>........] - ETA: 0s - loss: 58.6832 - accuracy: 0.3339
5728/7185 [======================>.......] - ETA: 0s - loss: 60.1635 - accuracy: 0.3359
6016/7185 [========================>.....] - ETA: 0s - loss: 66.7384 - accuracy: 0.3389
6336/7185 [=========================>....] - ETA: 0s - loss: 68.8435 - accuracy: 0.3365
6656/7185 [==========================>...] - ETA: 0s - loss: 69.0345 - accuracy: 0.3334
6976/7185 [============================>.] - ETA: 0s - loss: 71.2743 - accuracy: 0.3353
7185/7185 [==============================] - 1s 192us/step - loss: 69.3217 - accuracy: 0.3342 - val_loss: 33.0944 - val_accuracy: 0.3628

Umlaut results:
[<Warning: Learning Rate is high>, <Warning: Possible overfitting>]
Epoch 3/3

  32/7185 [..............................] - ETA: 1s - loss: 67.0525 - accuracy: 0.3750
 352/7185 [>.............................] - ETA: 1s - loss: 29.8681 - accuracy: 0.2983
 672/7185 [=>............................] - ETA: 1s - loss: 131.4293 - accuracy: 0.3482
 992/7185 [===>..........................] - ETA: 1s - loss: 107.7784 - accuracy: 0.3579
1312/7185 [====>.........................] - ETA: 0s - loss: 94.9557 - accuracy: 0.3651 
1600/7185 [=====>........................] - ETA: 0s - loss: 84.5512 - accuracy: 0.3631
1920/7185 [=======>......................] - ETA: 0s - loss: 71.7729 - accuracy: 0.3620
2240/7185 [========>.....................] - ETA: 0s - loss: 62.4857 - accuracy: 0.3496
2560/7185 [=========>....................] - ETA: 0s - loss: 54.9790 - accuracy: 0.3512
2880/7185 [===========>..................] - ETA: 0s - loss: 50.0647 - accuracy: 0.3438
3200/7185 [============>.................] - ETA: 0s - loss: 45.6292 - accuracy: 0.3459
3520/7185 [=============>................] - ETA: 0s - loss: 43.9757 - accuracy: 0.3463
3840/7185 [===============>..............] - ETA: 0s - loss: 41.5963 - accuracy: 0.3419
4160/7185 [================>.............] - ETA: 0s - loss: 38.5973 - accuracy: 0.3418
4480/7185 [=================>............] - ETA: 0s - loss: 44.8760 - accuracy: 0.3435
4768/7185 [==================>...........] - ETA: 0s - loss: 42.3301 - accuracy: 0.3423
5088/7185 [====================>.........] - ETA: 0s - loss: 39.9002 - accuracy: 0.3371
5408/7185 [=====================>........] - ETA: 0s - loss: 39.2875 - accuracy: 0.3376
5728/7185 [======================>.......] - ETA: 0s - loss: 37.4574 - accuracy: 0.3383
6016/7185 [========================>.....] - ETA: 0s - loss: 37.5464 - accuracy: 0.3378
6336/7185 [=========================>....] - ETA: 0s - loss: 36.7029 - accuracy: 0.3370
6656/7185 [==========================>...] - ETA: 0s - loss: 43.4264 - accuracy: 0.3392
6976/7185 [============================>.] - ETA: 0s - loss: 48.9349 - accuracy: 0.3405
7185/7185 [==============================] - 1s 191us/step - loss: 47.9153 - accuracy: 0.3420 - val_loss: 38.3416 - val_accuracy: 0.3656

Umlaut results:
[<Warning: Learning Rate is high>, <Warning: Possible overfitting>]

  32/2246 [..............................] - ETA: 0s
 512/2246 [=====>........................] - ETA: 0s
1056/2246 [=============>................] - ETA: 0s
1504/2246 [===================>..........] - ETA: 0s
1920/2246 [========================>.....] - ETA: 0s
2246/2246 [==============================] - 0s 111us/step
Test loss: 23.49290721435581
Test accuracy: 0.35752448439598083
D:\nargiz\github\umlaut\venvUMLT\lib\site-packages\keras\datasets\reuters.py:85: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])
D:\nargiz\github\umlaut\venvUMLT\lib\site-packages\keras\datasets\reuters.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])
# of Training Samples: 8982
# of Test Samples: 2246
# of Classes: 46
the an brazil vs reuter 68 vs 2 lt index countries 18 000 reuter all number 000 lt oper site consumption 000 reuter considerably reserves 000 an 784 vs reuter holiday vs some 100 hit our forward up indexes share countries 624 writedown index tonnes income a but reuter countries 624 627 a exchange half be 9 dlrs on bond 07 said in all 100 hit our japanese indexes two as before on tonnes said oper half be in hit our japanese pct dlrs
3
[0. 1. 0. ... 0. 0. 0.]
10000
[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
46
7185
mdl model.h5
['loss', 'accuracy']
Train on 7185 samples, validate on 1797 samples
Umlaut results:
[<Critical: Missing Softmax layer before loss>, <Warning: Last model layer has nonlinear activation>]
Epoch 1/3

  32/7185 [..............................] - ETA: 16s - loss: 3.8534 - accuracy: 0.0312
 288/7185 [>.............................] - ETA: 3s - loss: 601.8883 - accuracy: 0.3229
 544/7185 [=>............................] - ETA: 2s - loss: 388.5485 - accuracy: 0.3309
 800/7185 [==>...........................] - ETA: 1s - loss: 281.2966 - accuracy: 0.3487
1056/7185 [===>..........................] - ETA: 1s - loss: 217.9731 - accuracy: 0.3665
1312/7185 [====>.........................] - ETA: 1s - loss: 196.3824 - accuracy: 0.3765
1568/7185 [=====>........................] - ETA: 1s - loss: 169.5261 - accuracy: 0.3897
1824/7185 [======>.......................] - ETA: 1s - loss: 146.9864 - accuracy: 0.4002
2080/7185 [=======>......................] - ETA: 1s - loss: 131.3741 - accuracy: 0.3986
2368/7185 [========>.....................] - ETA: 1s - loss: 117.8027 - accuracy: 0.3932
2688/7185 [==========>...................] - ETA: 0s - loss: 105.8633 - accuracy: 0.3936
3008/7185 [===========>..................] - ETA: 0s - loss: 96.4297 - accuracy: 0.3916 
3328/7185 [============>.................] - ETA: 0s - loss: 88.4010 - accuracy: 0.3894
3648/7185 [==============>...............] - ETA: 0s - loss: 83.5707 - accuracy: 0.3882
3968/7185 [===============>..............] - ETA: 0s - loss: 77.2191 - accuracy: 0.3833
4288/7185 [================>.............] - ETA: 0s - loss: 71.6357 - accuracy: 0.3818
4608/7185 [==================>...........] - ETA: 0s - loss: 67.4101 - accuracy: 0.3828
4928/7185 [===================>..........] - ETA: 0s - loss: 63.5108 - accuracy: 0.3807
5248/7185 [====================>.........] - ETA: 0s - loss: 60.2741 - accuracy: 0.3760
5568/7185 [======================>.......] - ETA: 0s - loss: 57.3128 - accuracy: 0.3770
5888/7185 [=======================>......] - ETA: 0s - loss: 57.6234 - accuracy: 0.3782
6208/7185 [========================>.....] - ETA: 0s - loss: 55.6246 - accuracy: 0.3753
6496/7185 [==========================>...] - ETA: 0s - loss: 54.1644 - accuracy: 0.3742
6848/7185 [===========================>..] - ETA: 0s - loss: 53.4901 - accuracy: 0.3708
7185/7185 [==============================] - 2s 211us/step - loss: 51.1638 - accuracy: 0.3674 - val_loss: 23.9644 - val_accuracy: 0.3623

Umlaut results:
[<Warning: Learning Rate is high>]
Epoch 2/3

  32/7185 [..............................] - ETA: 1s - loss: 2.4521 - accuracy: 0.3125
 384/7185 [>.............................] - ETA: 1s - loss: 46.4604 - accuracy: 0.3438
 704/7185 [=>............................] - ETA: 1s - loss: 30.2603 - accuracy: 0.3466
1056/7185 [===>..........................] - ETA: 0s - loss: 29.8387 - accuracy: 0.3390
1408/7185 [====>.........................] - ETA: 0s - loss: 24.5231 - accuracy: 0.3594
1728/7185 [======>.......................] - ETA: 0s - loss: 26.7035 - accuracy: 0.3663
2048/7185 [=======>......................] - ETA: 0s - loss: 25.1424 - accuracy: 0.3555
2368/7185 [========>.....................] - ETA: 0s - loss: 28.4923 - accuracy: 0.3514
2688/7185 [==========>...................] - ETA: 0s - loss: 40.4913 - accuracy: 0.3471
3008/7185 [===========>..................] - ETA: 0s - loss: 45.1401 - accuracy: 0.3501
3328/7185 [============>.................] - ETA: 0s - loss: 43.8467 - accuracy: 0.3480
3648/7185 [==============>...............] - ETA: 0s - loss: 40.6581 - accuracy: 0.3473
3968/7185 [===============>..............] - ETA: 0s - loss: 40.5750 - accuracy: 0.3480
4288/7185 [================>.............] - ETA: 0s - loss: 39.0167 - accuracy: 0.3510
4608/7185 [==================>...........] - ETA: 0s - loss: 38.9323 - accuracy: 0.3546
4928/7185 [===================>..........] - ETA: 0s - loss: 36.9164 - accuracy: 0.3545
5248/7185 [====================>.........] - ETA: 0s - loss: 35.8695 - accuracy: 0.3550
5568/7185 [======================>.......] - ETA: 0s - loss: 35.5013 - accuracy: 0.3540
5920/7185 [=======================>......] - ETA: 0s - loss: 34.8847 - accuracy: 0.3500
6240/7185 [=========================>....] - ETA: 0s - loss: 33.5609 - accuracy: 0.3524
6560/7185 [==========================>...] - ETA: 0s - loss: 32.3865 - accuracy: 0.3517
6880/7185 [===========================>..] - ETA: 0s - loss: 30.9953 - accuracy: 0.3526
7185/7185 [==============================] - 1s 184us/step - loss: 30.2097 - accuracy: 0.3541 - val_loss: 17.3476 - val_accuracy: 0.4207

Umlaut results:
[<Warning: Learning Rate is high>]
Epoch 3/3

  32/7185 [..............................] - ETA: 1s - loss: 4.4931 - accuracy: 0.5938
 352/7185 [>.............................] - ETA: 1s - loss: 24.7896 - accuracy: 0.4006
 672/7185 [=>............................] - ETA: 1s - loss: 17.6515 - accuracy: 0.3616
 992/7185 [===>..........................] - ETA: 0s - loss: 57.5507 - accuracy: 0.3377
1344/7185 [====>.........................] - ETA: 0s - loss: 90.3611 - accuracy: 0.3415
1632/7185 [=====>........................] - ETA: 0s - loss: 78.8594 - accuracy: 0.3388
1920/7185 [=======>......................] - ETA: 0s - loss: 67.4657 - accuracy: 0.3438
2208/7185 [========>.....................] - ETA: 0s - loss: 66.2680 - accuracy: 0.3392
2528/7185 [=========>....................] - ETA: 0s - loss: 70.3745 - accuracy: 0.3430
2848/7185 [==========>...................] - ETA: 0s - loss: 66.9844 - accuracy: 0.3490
3168/7185 [============>.................] - ETA: 0s - loss: 61.3002 - accuracy: 0.3542
3488/7185 [=============>................] - ETA: 0s - loss: 56.3919 - accuracy: 0.3521
3840/7185 [===============>..............] - ETA: 0s - loss: 51.4765 - accuracy: 0.3547
4160/7185 [================>.............] - ETA: 0s - loss: 50.3725 - accuracy: 0.3570
4448/7185 [=================>............] - ETA: 0s - loss: 48.9364 - accuracy: 0.3566
4768/7185 [==================>...........] - ETA: 0s - loss: 47.7060 - accuracy: 0.3551
5120/7185 [====================>.........] - ETA: 0s - loss: 45.1731 - accuracy: 0.3545
5440/7185 [=====================>........] - ETA: 0s - loss: 46.4883 - accuracy: 0.3535
5760/7185 [=======================>......] - ETA: 0s - loss: 44.4255 - accuracy: 0.3533
6080/7185 [========================>.....] - ETA: 0s - loss: 42.4667 - accuracy: 0.3500
6400/7185 [=========================>....] - ETA: 0s - loss: 41.1346 - accuracy: 0.3505
6720/7185 [===========================>..] - ETA: 0s - loss: 39.7367 - accuracy: 0.3507
7072/7185 [============================>.] - ETA: 0s - loss: 40.6755 - accuracy: 0.3494
7185/7185 [==============================] - 1s 188us/step - loss: 40.1343 - accuracy: 0.3492 - val_loss: 9.4718 - val_accuracy: 0.3612

Umlaut results:
[<Warning: Learning Rate is high>]

  32/2246 [..............................] - ETA: 0s
 544/2246 [======>.......................] - ETA: 0s
1088/2246 [=============>................] - ETA: 0s
1664/2246 [=====================>........] - ETA: 0s
2240/2246 [============================>.] - ETA: 0s
2246/2246 [==============================] - 0s 94us/step
Test loss: 8.996245104610548
Test accuracy: 0.3566340208053589
D:\nargiz\github\umlaut\venvUMLT\lib\site-packages\keras\datasets\reuters.py:85: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])
D:\nargiz\github\umlaut\venvUMLT\lib\site-packages\keras\datasets\reuters.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])
# of Training Samples: 8982
# of Test Samples: 2246
# of Classes: 46
the an brazil vs reuter 68 vs 2 lt index countries 18 000 reuter all number 000 lt oper site consumption 000 reuter considerably reserves 000 an 784 vs reuter holiday vs some 100 hit our forward up indexes share countries 624 writedown index tonnes income a but reuter countries 624 627 a exchange half be 9 dlrs on bond 07 said in all 100 hit our japanese indexes two as before on tonnes said oper half be in hit our japanese pct dlrs
3
[0. 1. 0. ... 0. 0. 0.]
10000
[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
46
7185
mdl model.h5
['loss', 'accuracy']
Train on 7185 samples, validate on 1797 samples
Umlaut results:
[<Critical: Missing Softmax layer before loss>, <Warning: Last model layer has nonlinear activation>]
Epoch 1/3

  32/7185 [..............................] - ETA: 16s - loss: 3.7654 - accuracy: 0.0625
 288/7185 [>.............................] - ETA: 3s - loss: 636.0045 - accuracy: 0.3715
 544/7185 [=>............................] - ETA: 2s - loss: 371.0051 - accuracy: 0.3952
 800/7185 [==>...........................] - ETA: 1s - loss: 265.0217 - accuracy: 0.4125
1088/7185 [===>..........................] - ETA: 1s - loss: 211.4516 - accuracy: 0.4026
1344/7185 [====>.........................] - ETA: 1s - loss: 187.5852 - accuracy: 0.3981
1632/7185 [=====>........................] - ETA: 1s - loss: 160.7703 - accuracy: 0.3952
1920/7185 [=======>......................] - ETA: 1s - loss: 138.0405 - accuracy: 0.3969
2240/7185 [========>.....................] - ETA: 1s - loss: 119.6981 - accuracy: 0.3933
2592/7185 [=========>....................] - ETA: 0s - loss: 108.8580 - accuracy: 0.3989
2912/7185 [===========>..................] - ETA: 0s - loss: 101.4355 - accuracy: 0.4025
3232/7185 [============>.................] - ETA: 0s - loss: 92.7524 - accuracy: 0.4022 
3552/7185 [=============>................] - ETA: 0s - loss: 85.1781 - accuracy: 0.4003
3904/7185 [===============>..............] - ETA: 0s - loss: 79.6528 - accuracy: 0.4004
4224/7185 [================>.............] - ETA: 0s - loss: 73.9090 - accuracy: 0.3944
4512/7185 [=================>............] - ETA: 0s - loss: 69.5219 - accuracy: 0.3943
4832/7185 [===================>..........] - ETA: 0s - loss: 75.2194 - accuracy: 0.3893
5152/7185 [====================>.........] - ETA: 0s - loss: 72.7851 - accuracy: 0.3859
5472/7185 [=====================>........] - ETA: 0s - loss: 69.0743 - accuracy: 0.3808
5824/7185 [=======================>......] - ETA: 0s - loss: 65.1604 - accuracy: 0.3803
6144/7185 [========================>.....] - ETA: 0s - loss: 62.3427 - accuracy: 0.3820
6464/7185 [=========================>....] - ETA: 0s - loss: 59.4202 - accuracy: 0.3804
6784/7185 [===========================>..] - ETA: 0s - loss: 57.1685 - accuracy: 0.3782
7136/7185 [============================>.] - ETA: 0s - loss: 55.8789 - accuracy: 0.3767
7185/7185 [==============================] - 1s 208us/step - loss: 55.6096 - accuracy: 0.3765 - val_loss: 8.4147 - val_accuracy: 0.3573

Umlaut results:
[<Warning: Learning Rate is high>]
Epoch 2/3

  32/7185 [..............................] - ETA: 1s - loss: 2.3809 - accuracy: 0.4688
 352/7185 [>.............................] - ETA: 1s - loss: 5.5963 - accuracy: 0.3295
 704/7185 [=>............................] - ETA: 1s - loss: 10.1350 - accuracy: 0.3111
1056/7185 [===>..........................] - ETA: 0s - loss: 13.5821 - accuracy: 0.3381
1376/7185 [====>.........................] - ETA: 0s - loss: 36.0128 - accuracy: 0.3416
1696/7185 [======>.......................] - ETA: 0s - loss: 51.1334 - accuracy: 0.3608
2016/7185 [=======>......................] - ETA: 0s - loss: 68.5982 - accuracy: 0.3710
2336/7185 [========>.....................] - ETA: 0s - loss: 62.5785 - accuracy: 0.3848
2656/7185 [==========>...................] - ETA: 0s - loss: 60.1495 - accuracy: 0.3863
3008/7185 [===========>..................] - ETA: 0s - loss: 55.3326 - accuracy: 0.3813
3328/7185 [============>.................] - ETA: 0s - loss: 72.4214 - accuracy: 0.3720
3648/7185 [==============>...............] - ETA: 0s - loss: 71.0506 - accuracy: 0.3698
3968/7185 [===============>..............] - ETA: 0s - loss: 69.4439 - accuracy: 0.3710
4320/7185 [=================>............] - ETA: 0s - loss: 69.3055 - accuracy: 0.3694
4640/7185 [==================>...........] - ETA: 0s - loss: 69.8650 - accuracy: 0.3666
4960/7185 [===================>..........] - ETA: 0s - loss: 68.5364 - accuracy: 0.3675
5280/7185 [=====================>........] - ETA: 0s - loss: 79.2122 - accuracy: 0.3688
5632/7185 [======================>.......] - ETA: 0s - loss: 78.2240 - accuracy: 0.3677
5984/7185 [=======================>......] - ETA: 0s - loss: 81.1195 - accuracy: 0.3650
6304/7185 [=========================>....] - ETA: 0s - loss: 78.5894 - accuracy: 0.3617
6624/7185 [==========================>...] - ETA: 0s - loss: 75.8290 - accuracy: 0.3625
6944/7185 [===========================>..] - ETA: 0s - loss: 73.2412 - accuracy: 0.3619
7185/7185 [==============================] - 1s 186us/step - loss: 71.0100 - accuracy: 0.3602 - val_loss: 11.3289 - val_accuracy: 0.3684

Umlaut results:
[<Warning: Learning Rate is high>]
Epoch 3/3

  32/7185 [..............................] - ETA: 1s - loss: 79.8455 - accuracy: 0.3750
 384/7185 [>.............................] - ETA: 1s - loss: 75.6306 - accuracy: 0.3880
 736/7185 [==>...........................] - ETA: 0s - loss: 96.7752 - accuracy: 0.3668
1088/7185 [===>..........................] - ETA: 0s - loss: 71.9938 - accuracy: 0.3438
1440/7185 [=====>........................] - ETA: 0s - loss: 76.9137 - accuracy: 0.3410
1728/7185 [======>.......................] - ETA: 0s - loss: 71.5794 - accuracy: 0.3380
2016/7185 [=======>......................] - ETA: 0s - loss: 74.4937 - accuracy: 0.3348
2336/7185 [========>.....................] - ETA: 0s - loss: 71.2960 - accuracy: 0.3348
2656/7185 [==========>...................] - ETA: 0s - loss: 64.3883 - accuracy: 0.3347
2976/7185 [===========>..................] - ETA: 0s - loss: 69.7274 - accuracy: 0.3404
3264/7185 [============>.................] - ETA: 0s - loss: 65.2194 - accuracy: 0.3327
3584/7185 [=============>................] - ETA: 0s - loss: 65.0789 - accuracy: 0.3365
3936/7185 [===============>..............] - ETA: 0s - loss: 62.9515 - accuracy: 0.3382
4288/7185 [================>.............] - ETA: 0s - loss: 58.0623 - accuracy: 0.3377
4640/7185 [==================>...........] - ETA: 0s - loss: 54.7433 - accuracy: 0.3392
4992/7185 [===================>..........] - ETA: 0s - loss: 54.2735 - accuracy: 0.3375
5312/7185 [=====================>........] - ETA: 0s - loss: 60.3129 - accuracy: 0.3355
5632/7185 [======================>.......] - ETA: 0s - loss: 61.0966 - accuracy: 0.3349
5952/7185 [=======================>......] - ETA: 0s - loss: 63.0816 - accuracy: 0.3327
6272/7185 [=========================>....] - ETA: 0s - loss: 60.5077 - accuracy: 0.3329
6624/7185 [==========================>...] - ETA: 0s - loss: 64.3090 - accuracy: 0.3317
6944/7185 [===========================>..] - ETA: 0s - loss: 61.5117 - accuracy: 0.3325
7185/7185 [==============================] - 1s 186us/step - loss: 60.8239 - accuracy: 0.3344 - val_loss: 15.1012 - val_accuracy: 0.3584

Umlaut results:
[<Warning: Learning Rate is high>, <Warning: Possible overfitting>]

  32/2246 [..............................] - ETA: 0s
 576/2246 [======>.......................] - ETA: 0s
1088/2246 [=============>................] - ETA: 0s
1600/2246 [====================>.........] - ETA: 0s
2144/2246 [===========================>..] - ETA: 0s
2246/2246 [==============================] - 0s 100us/step
Test loss: 8.260359046508029
Test accuracy: 0.36019590497016907
D:\nargiz\github\umlaut\venvUMLT\lib\site-packages\keras\datasets\reuters.py:85: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])
D:\nargiz\github\umlaut\venvUMLT\lib\site-packages\keras\datasets\reuters.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])
# of Training Samples: 8982
# of Test Samples: 2246
# of Classes: 46
the an brazil vs reuter 68 vs 2 lt index countries 18 000 reuter all number 000 lt oper site consumption 000 reuter considerably reserves 000 an 784 vs reuter holiday vs some 100 hit our forward up indexes share countries 624 writedown index tonnes income a but reuter countries 624 627 a exchange half be 9 dlrs on bond 07 said in all 100 hit our japanese indexes two as before on tonnes said oper half be in hit our japanese pct dlrs
3
[0. 1. 0. ... 0. 0. 0.]
10000
[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
46
7185
mdl model.h5
['loss', 'accuracy']
Train on 7185 samples, validate on 1797 samples
Umlaut results:
[<Critical: Missing Softmax layer before loss>, <Warning: Last model layer has nonlinear activation>]
Epoch 1/3

  32/7185 [..............................] - ETA: 16s - loss: 3.8310 - accuracy: 0.0000e+00
 288/7185 [>.............................] - ETA: 3s - loss: 452.8626 - accuracy: 0.4375   
 544/7185 [=>............................] - ETA: 2s - loss: 276.8143 - accuracy: 0.4026
 832/7185 [==>...........................] - ETA: 1s - loss: 238.1597 - accuracy: 0.4171
1120/7185 [===>..........................] - ETA: 1s - loss: 202.7498 - accuracy: 0.4018
1408/7185 [====>.........................] - ETA: 1s - loss: 169.5934 - accuracy: 0.4055
1728/7185 [======>.......................] - ETA: 1s - loss: 151.6577 - accuracy: 0.4039
2048/7185 [=======>......................] - ETA: 1s - loss: 130.4403 - accuracy: 0.4092
2368/7185 [========>.....................] - ETA: 1s - loss: 116.1089 - accuracy: 0.4071
2720/7185 [==========>...................] - ETA: 0s - loss: 107.0005 - accuracy: 0.4129
3040/7185 [===========>..................] - ETA: 0s - loss: 97.2862 - accuracy: 0.4092 
3360/7185 [=============>................] - ETA: 0s - loss: 94.4729 - accuracy: 0.4122
3712/7185 [==============>...............] - ETA: 0s - loss: 90.0746 - accuracy: 0.4089
4064/7185 [===============>..............] - ETA: 0s - loss: 83.1601 - accuracy: 0.4067
4384/7185 [=================>............] - ETA: 0s - loss: 78.9831 - accuracy: 0.4053
4704/7185 [==================>...........] - ETA: 0s - loss: 74.0941 - accuracy: 0.4028
5024/7185 [===================>..........] - ETA: 0s - loss: 70.9735 - accuracy: 0.4001
5376/7185 [=====================>........] - ETA: 0s - loss: 67.7107 - accuracy: 0.3958
5696/7185 [======================>.......] - ETA: 0s - loss: 72.5080 - accuracy: 0.3882
6016/7185 [========================>.....] - ETA: 0s - loss: 69.8536 - accuracy: 0.3878
6336/7185 [=========================>....] - ETA: 0s - loss: 68.8392 - accuracy: 0.3853
6656/7185 [==========================>...] - ETA: 0s - loss: 65.9392 - accuracy: 0.3815
7008/7185 [============================>.] - ETA: 0s - loss: 63.2585 - accuracy: 0.3769
7185/7185 [==============================] - 1s 203us/step - loss: 61.9262 - accuracy: 0.3738 - val_loss: 47.2939 - val_accuracy: 0.3600

Umlaut results:
[<Warning: Learning Rate is high>]
Epoch 2/3

  32/7185 [..............................] - ETA: 1s - loss: 1184.2006 - accuracy: 0.2812
 384/7185 [>.............................] - ETA: 1s - loss: 155.5971 - accuracy: 0.3464 
 736/7185 [==>...........................] - ETA: 0s - loss: 103.7881 - accuracy: 0.3777
1088/7185 [===>..........................] - ETA: 0s - loss: 81.8744 - accuracy: 0.3732 
1440/7185 [=====>........................] - ETA: 0s - loss: 75.8866 - accuracy: 0.3660
1760/7185 [======>.......................] - ETA: 0s - loss: 68.8517 - accuracy: 0.3580
2080/7185 [=======>......................] - ETA: 0s - loss: 99.3675 - accuracy: 0.3587
2400/7185 [=========>....................] - ETA: 0s - loss: 93.1687 - accuracy: 0.3658
2720/7185 [==========>...................] - ETA: 0s - loss: 89.5790 - accuracy: 0.3713
3040/7185 [===========>..................] - ETA: 0s - loss: 86.5845 - accuracy: 0.3720
3360/7185 [=============>................] - ETA: 0s - loss: 79.8416 - accuracy: 0.3652
3712/7185 [==============>...............] - ETA: 0s - loss: 73.3062 - accuracy: 0.3640
4064/7185 [===============>..............] - ETA: 0s - loss: 67.1847 - accuracy: 0.3602
4384/7185 [=================>............] - ETA: 0s - loss: 63.1276 - accuracy: 0.3572
4704/7185 [==================>...........] - ETA: 0s - loss: 61.3310 - accuracy: 0.3557
5024/7185 [===================>..........] - ETA: 0s - loss: 58.7681 - accuracy: 0.3529
5376/7185 [=====================>........] - ETA: 0s - loss: 71.1305 - accuracy: 0.3499
5696/7185 [======================>.......] - ETA: 0s - loss: 74.5451 - accuracy: 0.3495
6016/7185 [========================>.....] - ETA: 0s - loss: 72.9093 - accuracy: 0.3497
6368/7185 [=========================>....] - ETA: 0s - loss: 90.1512 - accuracy: 0.3505
6688/7185 [==========================>...] - ETA: 0s - loss: 86.3211 - accuracy: 0.3520
7040/7185 [============================>.] - ETA: 0s - loss: 82.3944 - accuracy: 0.3511
7185/7185 [==============================] - 1s 181us/step - loss: 90.5009 - accuracy: 0.3493 - val_loss: 54.5659 - val_accuracy: 0.3328

Umlaut results:
[<Warning: Learning Rate is high>]
Epoch 3/3

  32/7185 [..............................] - ETA: 1s - loss: 2.4414 - accuracy: 0.4062
 384/7185 [>.............................] - ETA: 1s - loss: 28.9510 - accuracy: 0.3828
 704/7185 [=>............................] - ETA: 1s - loss: 27.3559 - accuracy: 0.3693
1056/7185 [===>..........................] - ETA: 0s - loss: 23.3475 - accuracy: 0.3712
1408/7185 [====>.........................] - ETA: 0s - loss: 31.1791 - accuracy: 0.3544
1696/7185 [======>.......................] - ETA: 0s - loss: 27.3142 - accuracy: 0.3367
1984/7185 [=======>......................] - ETA: 0s - loss: 23.6950 - accuracy: 0.3352
2304/7185 [========>.....................] - ETA: 0s - loss: 21.7152 - accuracy: 0.3346
2656/7185 [==========>...................] - ETA: 0s - loss: 34.4570 - accuracy: 0.3370
2976/7185 [===========>..................] - ETA: 0s - loss: 87.2294 - accuracy: 0.3340
3328/7185 [============>.................] - ETA: 0s - loss: 107.8382 - accuracy: 0.3338
3648/7185 [==============>...............] - ETA: 0s - loss: 100.4049 - accuracy: 0.3284
3968/7185 [===============>..............] - ETA: 0s - loss: 101.0240 - accuracy: 0.3256
4320/7185 [=================>............] - ETA: 0s - loss: 95.7918 - accuracy: 0.3326 
4640/7185 [==================>...........] - ETA: 0s - loss: 89.5538 - accuracy: 0.3284
4992/7185 [===================>..........] - ETA: 0s - loss: 83.5822 - accuracy: 0.3341
5312/7185 [=====================>........] - ETA: 0s - loss: 115.3622 - accuracy: 0.3347
5664/7185 [======================>.......] - ETA: 0s - loss: 113.2543 - accuracy: 0.3321
6016/7185 [========================>.....] - ETA: 0s - loss: 108.6221 - accuracy: 0.3333
6368/7185 [=========================>....] - ETA: 0s - loss: 105.9494 - accuracy: 0.3318
6688/7185 [==========================>...] - ETA: 0s - loss: 101.2888 - accuracy: 0.3324
7008/7185 [============================>.] - ETA: 0s - loss: 97.2471 - accuracy: 0.3315 
7185/7185 [==============================] - 1s 182us/step - loss: 97.0033 - accuracy: 0.3300 - val_loss: 20.9135 - val_accuracy: 0.3612

Umlaut results:
[<Warning: Learning Rate is high>]

  32/2246 [..............................] - ETA: 0s
 544/2246 [======>.......................] - ETA: 0s
1024/2246 [============>.................] - ETA: 0s
1536/2246 [===================>..........] - ETA: 0s
2016/2246 [=========================>....] - ETA: 0s
2246/2246 [==============================] - 0s 105us/step
Test loss: 68.04931776181779
Test accuracy: 0.3584149479866028
D:\nargiz\github\umlaut\venvUMLT\lib\site-packages\keras\datasets\reuters.py:85: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])
D:\nargiz\github\umlaut\venvUMLT\lib\site-packages\keras\datasets\reuters.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])
# of Training Samples: 8982
# of Test Samples: 2246
# of Classes: 46
the an brazil vs reuter 68 vs 2 lt index countries 18 000 reuter all number 000 lt oper site consumption 000 reuter considerably reserves 000 an 784 vs reuter holiday vs some 100 hit our forward up indexes share countries 624 writedown index tonnes income a but reuter countries 624 627 a exchange half be 9 dlrs on bond 07 said in all 100 hit our japanese indexes two as before on tonnes said oper half be in hit our japanese pct dlrs
3
[0. 1. 0. ... 0. 0. 0.]
10000
[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
46
7185
mdl model.h5
['loss', 'accuracy']
Train on 7185 samples, validate on 1797 samples
Umlaut results:
[<Critical: Missing Softmax layer before loss>, <Warning: Last model layer has nonlinear activation>]
Epoch 1/3

  32/7185 [..............................] - ETA: 15s - loss: 3.8610 - accuracy: 0.0000e+00
 256/7185 [>.............................] - ETA: 3s - loss: 1387.2226 - accuracy: 0.2266  
 512/7185 [=>............................] - ETA: 2s - loss: 784.0705 - accuracy: 0.2988 
 768/7185 [==>...........................] - ETA: 1s - loss: 552.2796 - accuracy: 0.3672
1024/7185 [===>..........................] - ETA: 1s - loss: 437.8743 - accuracy: 0.3516
1280/7185 [====>.........................] - ETA: 1s - loss: 357.2185 - accuracy: 0.3422
1536/7185 [=====>........................] - ETA: 1s - loss: 304.2814 - accuracy: 0.3568
1824/7185 [======>.......................] - ETA: 1s - loss: 262.6206 - accuracy: 0.3673
2112/7185 [=======>......................] - ETA: 1s - loss: 228.0423 - accuracy: 0.3698
2368/7185 [========>.....................] - ETA: 1s - loss: 209.1328 - accuracy: 0.3695
2688/7185 [==========>...................] - ETA: 1s - loss: 185.8456 - accuracy: 0.3735
3008/7185 [===========>..................] - ETA: 0s - loss: 167.3827 - accuracy: 0.3700
3328/7185 [============>.................] - ETA: 0s - loss: 153.8657 - accuracy: 0.3660
3648/7185 [==============>...............] - ETA: 0s - loss: 141.5904 - accuracy: 0.3712
3936/7185 [===============>..............] - ETA: 0s - loss: 133.2353 - accuracy: 0.3730
4256/7185 [================>.............] - ETA: 0s - loss: 123.8837 - accuracy: 0.3694
4576/7185 [==================>...........] - ETA: 0s - loss: 115.6641 - accuracy: 0.3706
4928/7185 [===================>..........] - ETA: 0s - loss: 108.1180 - accuracy: 0.3697
5248/7185 [====================>.........] - ETA: 0s - loss: 101.8109 - accuracy: 0.3645
5568/7185 [======================>.......] - ETA: 0s - loss: 96.1512 - accuracy: 0.3678 
5856/7185 [=======================>......] - ETA: 0s - loss: 91.6082 - accuracy: 0.3644
6144/7185 [========================>.....] - ETA: 0s - loss: 88.1058 - accuracy: 0.3634
6464/7185 [=========================>....] - ETA: 0s - loss: 88.4552 - accuracy: 0.3605
6784/7185 [===========================>..] - ETA: 0s - loss: 87.5947 - accuracy: 0.3626
7104/7185 [============================>.] - ETA: 0s - loss: 84.1733 - accuracy: 0.3622
7185/7185 [==============================] - 2s 216us/step - loss: 83.2544 - accuracy: 0.3620 - val_loss: 11.3759 - val_accuracy: 0.3578

Umlaut results:
[<Warning: Learning Rate is high>]
Epoch 2/3

  32/7185 [..............................] - ETA: 1s - loss: 2.2142 - accuracy: 0.4688
 352/7185 [>.............................] - ETA: 1s - loss: 16.6655 - accuracy: 0.3324
 672/7185 [=>............................] - ETA: 1s - loss: 37.9225 - accuracy: 0.3244
1024/7185 [===>..........................] - ETA: 0s - loss: 105.6916 - accuracy: 0.3428
1376/7185 [====>.........................] - ETA: 0s - loss: 88.8641 - accuracy: 0.3539 
1696/7185 [======>.......................] - ETA: 0s - loss: 78.3394 - accuracy: 0.3555
2016/7185 [=======>......................] - ETA: 0s - loss: 68.8336 - accuracy: 0.3477
2336/7185 [========>.....................] - ETA: 0s - loss: 62.8957 - accuracy: 0.3515
2656/7185 [==========>...................] - ETA: 0s - loss: 60.1520 - accuracy: 0.3524
2976/7185 [===========>..................] - ETA: 0s - loss: 57.4772 - accuracy: 0.3538
3296/7185 [============>.................] - ETA: 0s - loss: 54.7599 - accuracy: 0.3553
3616/7185 [==============>...............] - ETA: 0s - loss: 53.4299 - accuracy: 0.3537
3936/7185 [===============>..............] - ETA: 0s - loss: 52.4971 - accuracy: 0.3501
4256/7185 [================>.............] - ETA: 0s - loss: 59.7662 - accuracy: 0.3494
4576/7185 [==================>...........] - ETA: 0s - loss: 74.1313 - accuracy: 0.3444
4896/7185 [===================>..........] - ETA: 0s - loss: 75.2863 - accuracy: 0.3399
5216/7185 [====================>.........] - ETA: 0s - loss: 71.4974 - accuracy: 0.3370
5536/7185 [======================>.......] - ETA: 0s - loss: 67.9793 - accuracy: 0.3374
5856/7185 [=======================>......] - ETA: 0s - loss: 68.5672 - accuracy: 0.3367
6176/7185 [========================>.....] - ETA: 0s - loss: 66.3970 - accuracy: 0.3387
6496/7185 [==========================>...] - ETA: 0s - loss: 64.8784 - accuracy: 0.3390
6784/7185 [===========================>..] - ETA: 0s - loss: 63.8674 - accuracy: 0.3398
7104/7185 [============================>.] - ETA: 0s - loss: 61.8309 - accuracy: 0.3405
7185/7185 [==============================] - 1s 190us/step - loss: 61.6942 - accuracy: 0.3410 - val_loss: 9.0275 - val_accuracy: 0.3166

Umlaut results:
[<Warning: Learning Rate is high>]
Epoch 3/3

  32/7185 [..............................] - ETA: 1s - loss: 2.2768 - accuracy: 0.2500
 352/7185 [>.............................] - ETA: 1s - loss: 14.1802 - accuracy: 0.3409
 672/7185 [=>............................] - ETA: 1s - loss: 28.3137 - accuracy: 0.3393
 992/7185 [===>..........................] - ETA: 1s - loss: 50.2549 - accuracy: 0.3266
1312/7185 [====>.........................] - ETA: 0s - loss: 70.4474 - accuracy: 0.3216
1600/7185 [=====>........................] - ETA: 0s - loss: 77.1686 - accuracy: 0.3200
1920/7185 [=======>......................] - ETA: 0s - loss: 66.9352 - accuracy: 0.3255
2208/7185 [========>.....................] - ETA: 0s - loss: 70.2718 - accuracy: 0.3211
2528/7185 [=========>....................] - ETA: 0s - loss: 67.2032 - accuracy: 0.3208
2848/7185 [==========>...................] - ETA: 0s - loss: 71.0394 - accuracy: 0.3223
3168/7185 [============>.................] - ETA: 0s - loss: 64.7975 - accuracy: 0.3191
3488/7185 [=============>................] - ETA: 0s - loss: 60.2026 - accuracy: 0.3217
3808/7185 [==============>...............] - ETA: 0s - loss: 62.3085 - accuracy: 0.3185
4128/7185 [================>.............] - ETA: 0s - loss: 60.7787 - accuracy: 0.3215
4448/7185 [=================>............] - ETA: 0s - loss: 59.0693 - accuracy: 0.3235
4768/7185 [==================>...........] - ETA: 0s - loss: 55.2733 - accuracy: 0.3270
5056/7185 [====================>.........] - ETA: 0s - loss: 57.2812 - accuracy: 0.3297
5344/7185 [=====================>........] - ETA: 0s - loss: 56.1236 - accuracy: 0.3325
5664/7185 [======================>.......] - ETA: 0s - loss: 54.0632 - accuracy: 0.3325
5952/7185 [=======================>......] - ETA: 0s - loss: 68.8808 - accuracy: 0.3335
6272/7185 [=========================>....] - ETA: 0s - loss: 66.3986 - accuracy: 0.3374
6560/7185 [==========================>...] - ETA: 0s - loss: 74.9866 - accuracy: 0.3383
6880/7185 [===========================>..] - ETA: 0s - loss: 73.1263 - accuracy: 0.3384
7185/7185 [==============================] - 1s 193us/step - loss: 72.1154 - accuracy: 0.3363 - val_loss: 8.8398 - val_accuracy: 0.3578

Umlaut results:
[<Warning: Learning Rate is high>]

  32/2246 [..............................] - ETA: 0s
 512/2246 [=====>........................] - ETA: 0s
1024/2246 [============>.................] - ETA: 0s
1472/2246 [==================>...........] - ETA: 0s
1984/2246 [=========================>....] - ETA: 0s
2246/2246 [==============================] - 0s 106us/step
Test loss: 8.705609082963354
Test accuracy: 0.35396260023117065
D:\nargiz\github\umlaut\venvUMLT\lib\site-packages\keras\datasets\reuters.py:85: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])
D:\nargiz\github\umlaut\venvUMLT\lib\site-packages\keras\datasets\reuters.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])
# of Training Samples: 8982
# of Test Samples: 2246
# of Classes: 46
the an brazil vs reuter 68 vs 2 lt index countries 18 000 reuter all number 000 lt oper site consumption 000 reuter considerably reserves 000 an 784 vs reuter holiday vs some 100 hit our forward up indexes share countries 624 writedown index tonnes income a but reuter countries 624 627 a exchange half be 9 dlrs on bond 07 said in all 100 hit our japanese indexes two as before on tonnes said oper half be in hit our japanese pct dlrs
3
[0. 1. 0. ... 0. 0. 0.]
10000
[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
46
7185
mdl model.h5
['loss', 'accuracy']
Train on 7185 samples, validate on 1797 samples
Umlaut results:
[<Critical: Missing Softmax layer before loss>, <Warning: Last model layer has nonlinear activation>]
Epoch 1/3

  32/7185 [..............................] - ETA: 16s - loss: 3.8779 - accuracy: 0.0000e+00
 256/7185 [>.............................] - ETA: 3s - loss: 857.8527 - accuracy: 0.2422   
 512/7185 [=>............................] - ETA: 2s - loss: 454.7248 - accuracy: 0.3203
 768/7185 [==>...........................] - ETA: 1s - loss: 340.5197 - accuracy: 0.3828
1056/7185 [===>..........................] - ETA: 1s - loss: 262.0056 - accuracy: 0.4025
1312/7185 [====>.........................] - ETA: 1s - loss: 221.0212 - accuracy: 0.3956
1568/7185 [=====>........................] - ETA: 1s - loss: 188.2121 - accuracy: 0.4062
1824/7185 [======>.......................] - ETA: 1s - loss: 165.5661 - accuracy: 0.4150
2112/7185 [=======>......................] - ETA: 1s - loss: 145.0655 - accuracy: 0.4276
2400/7185 [=========>....................] - ETA: 1s - loss: 143.2661 - accuracy: 0.4304
2688/7185 [==========>...................] - ETA: 1s - loss: 129.5855 - accuracy: 0.4327
3008/7185 [===========>..................] - ETA: 0s - loss: 121.7647 - accuracy: 0.4332
3328/7185 [============>.................] - ETA: 0s - loss: 117.5636 - accuracy: 0.4267
3648/7185 [==============>...............] - ETA: 0s - loss: 108.6273 - accuracy: 0.4180
3968/7185 [===============>..............] - ETA: 0s - loss: 101.8977 - accuracy: 0.4166
4288/7185 [================>.............] - ETA: 0s - loss: 96.2860 - accuracy: 0.4139 
4608/7185 [==================>...........] - ETA: 0s - loss: 93.0258 - accuracy: 0.4128
4960/7185 [===================>..........] - ETA: 0s - loss: 87.2538 - accuracy: 0.4050
5280/7185 [=====================>........] - ETA: 0s - loss: 88.9452 - accuracy: 0.4002
5600/7185 [======================>.......] - ETA: 0s - loss: 85.7019 - accuracy: 0.3963
5888/7185 [=======================>......] - ETA: 0s - loss: 82.2572 - accuracy: 0.3927
6208/7185 [========================>.....] - ETA: 0s - loss: 80.2556 - accuracy: 0.3871
6528/7185 [==========================>...] - ETA: 0s - loss: 76.8152 - accuracy: 0.3836
6848/7185 [===========================>..] - ETA: 0s - loss: 73.7768 - accuracy: 0.3805
7168/7185 [============================>.] - ETA: 0s - loss: 71.4959 - accuracy: 0.3810
7185/7185 [==============================] - 2s 213us/step - loss: 71.3325 - accuracy: 0.3808 - val_loss: 37.0340 - val_accuracy: 0.3556

Umlaut results:
[<Warning: Learning Rate is high>]
Epoch 2/3

  32/7185 [..............................] - ETA: 1s - loss: 15.3618 - accuracy: 0.3125
 384/7185 [>.............................] - ETA: 1s - loss: 15.1009 - accuracy: 0.3906
 704/7185 [=>............................] - ETA: 1s - loss: 20.2168 - accuracy: 0.3864
1056/7185 [===>..........................] - ETA: 0s - loss: 32.1761 - accuracy: 0.3712
1408/7185 [====>.........................] - ETA: 0s - loss: 33.4422 - accuracy: 0.3700
1696/7185 [======>.......................] - ETA: 0s - loss: 28.8872 - accuracy: 0.3715
1984/7185 [=======>......................] - ETA: 0s - loss: 34.9435 - accuracy: 0.3770
2304/7185 [========>.....................] - ETA: 0s - loss: 38.8015 - accuracy: 0.3694
2656/7185 [==========>...................] - ETA: 0s - loss: 37.0779 - accuracy: 0.3626
2976/7185 [===========>..................] - ETA: 0s - loss: 43.3634 - accuracy: 0.3558
3296/7185 [============>.................] - ETA: 0s - loss: 41.9334 - accuracy: 0.3571
3616/7185 [==============>...............] - ETA: 0s - loss: 40.7538 - accuracy: 0.3548
3936/7185 [===============>..............] - ETA: 0s - loss: 40.5630 - accuracy: 0.3519
4256/7185 [================>.............] - ETA: 0s - loss: 42.7352 - accuracy: 0.3510
4608/7185 [==================>...........] - ETA: 0s - loss: 54.9930 - accuracy: 0.3529
4928/7185 [===================>..........] - ETA: 0s - loss: 56.6318 - accuracy: 0.3513
5248/7185 [====================>.........] - ETA: 0s - loss: 55.1205 - accuracy: 0.3548
5568/7185 [======================>.......] - ETA: 0s - loss: 74.3190 - accuracy: 0.3531
5920/7185 [=======================>......] - ETA: 0s - loss: 71.9265 - accuracy: 0.3500
6240/7185 [=========================>....] - ETA: 0s - loss: 70.8151 - accuracy: 0.3508
6560/7185 [==========================>...] - ETA: 0s - loss: 68.1289 - accuracy: 0.3526
6880/7185 [===========================>..] - ETA: 0s - loss: 65.5615 - accuracy: 0.3531
7185/7185 [==============================] - 1s 185us/step - loss: 64.5759 - accuracy: 0.3514 - val_loss: 70.9919 - val_accuracy: 0.3511

Umlaut results:
[<Warning: Learning Rate is high>, <Warning: Possible overfitting>]
Epoch 3/3

  32/7185 [..............................] - ETA: 1s - loss: 38.1149 - accuracy: 0.3750
 352/7185 [>.............................] - ETA: 1s - loss: 18.0823 - accuracy: 0.3551
 672/7185 [=>............................] - ETA: 1s - loss: 25.2998 - accuracy: 0.3229
1024/7185 [===>..........................] - ETA: 0s - loss: 447.8172 - accuracy: 0.3291
1344/7185 [====>.........................] - ETA: 0s - loss: 350.6119 - accuracy: 0.3251
1664/7185 [=====>........................] - ETA: 0s - loss: 287.2055 - accuracy: 0.3407
1984/7185 [=======>......................] - ETA: 0s - loss: 254.0401 - accuracy: 0.3372
2304/7185 [========>.....................] - ETA: 0s - loss: 230.0635 - accuracy: 0.3359
2624/7185 [=========>....................] - ETA: 0s - loss: 204.6634 - accuracy: 0.3319
2944/7185 [===========>..................] - ETA: 0s - loss: 182.7356 - accuracy: 0.3349
3264/7185 [============>.................] - ETA: 0s - loss: 165.4225 - accuracy: 0.3336
3584/7185 [=============>................] - ETA: 0s - loss: 159.5547 - accuracy: 0.3318
3904/7185 [===============>..............] - ETA: 0s - loss: 226.5806 - accuracy: 0.3335
4256/7185 [================>.............] - ETA: 0s - loss: 213.4581 - accuracy: 0.3292
4576/7185 [==================>...........] - ETA: 0s - loss: 198.8409 - accuracy: 0.3298
4896/7185 [===================>..........] - ETA: 0s - loss: 186.7678 - accuracy: 0.3333
5248/7185 [====================>.........] - ETA: 0s - loss: 175.0565 - accuracy: 0.3357
5600/7185 [======================>.......] - ETA: 0s - loss: 164.2991 - accuracy: 0.3346
5920/7185 [=======================>......] - ETA: 0s - loss: 155.8436 - accuracy: 0.3348
6240/7185 [=========================>....] - ETA: 0s - loss: 148.7918 - accuracy: 0.3354
6560/7185 [==========================>...] - ETA: 0s - loss: 142.0489 - accuracy: 0.3332
6912/7185 [===========================>..] - ETA: 0s - loss: 135.1376 - accuracy: 0.3343
7185/7185 [==============================] - 1s 185us/step - loss: 130.1075 - accuracy: 0.3340 - val_loss: 5.4902 - val_accuracy: 0.3612

Umlaut results:
[<Warning: Learning Rate is high>]

  32/2246 [..............................] - ETA: 0s
 544/2246 [======>.......................] - ETA: 0s
1056/2246 [=============>................] - ETA: 0s
1568/2246 [===================>..........] - ETA: 0s
2080/2246 [==========================>...] - ETA: 0s
2246/2246 [==============================] - 0s 102us/step
Test loss: 13.402162754206602
Test accuracy: 0.35529831051826477
D:\nargiz\github\umlaut\venvUMLT\lib\site-packages\keras\datasets\reuters.py:85: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])
D:\nargiz\github\umlaut\venvUMLT\lib\site-packages\keras\datasets\reuters.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])
# of Training Samples: 8982
# of Test Samples: 2246
# of Classes: 46
the an brazil vs reuter 68 vs 2 lt index countries 18 000 reuter all number 000 lt oper site consumption 000 reuter considerably reserves 000 an 784 vs reuter holiday vs some 100 hit our forward up indexes share countries 624 writedown index tonnes income a but reuter countries 624 627 a exchange half be 9 dlrs on bond 07 said in all 100 hit our japanese indexes two as before on tonnes said oper half be in hit our japanese pct dlrs
3
[0. 1. 0. ... 0. 0. 0.]
10000
[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
46
7185
mdl model.h5
['loss', 'accuracy']
Train on 7185 samples, validate on 1797 samples
Umlaut results:
[<Critical: Missing Softmax layer before loss>, <Warning: Last model layer has nonlinear activation>]
Epoch 1/3

  32/7185 [..............................] - ETA: 16s - loss: 3.8291 - accuracy: 0.0625
 288/7185 [>.............................] - ETA: 3s - loss: 1120.2943 - accuracy: 0.3056
 544/7185 [=>............................] - ETA: 2s - loss: 616.8473 - accuracy: 0.3382 
 832/7185 [==>...........................] - ETA: 1s - loss: 441.0940 - accuracy: 0.3750
1088/7185 [===>..........................] - ETA: 1s - loss: 341.7277 - accuracy: 0.3759
1344/7185 [====>.........................] - ETA: 1s - loss: 278.2428 - accuracy: 0.3854
1600/7185 [=====>........................] - ETA: 1s - loss: 237.4774 - accuracy: 0.4050
1888/7185 [======>.......................] - ETA: 1s - loss: 209.3523 - accuracy: 0.3978
2176/7185 [========>.....................] - ETA: 1s - loss: 185.7693 - accuracy: 0.3897
2464/7185 [=========>....................] - ETA: 1s - loss: 165.0080 - accuracy: 0.3904
2784/7185 [==========>...................] - ETA: 0s - loss: 152.6215 - accuracy: 0.3865
3104/7185 [===========>..................] - ETA: 0s - loss: 139.4940 - accuracy: 0.3773
3424/7185 [=============>................] - ETA: 0s - loss: 127.4761 - accuracy: 0.3724
3744/7185 [==============>...............] - ETA: 0s - loss: 117.1358 - accuracy: 0.3729
4064/7185 [===============>..............] - ETA: 0s - loss: 109.4268 - accuracy: 0.3688
4384/7185 [=================>............] - ETA: 0s - loss: 102.9720 - accuracy: 0.3659
4736/7185 [==================>...........] - ETA: 0s - loss: 96.1245 - accuracy: 0.3661 
5056/7185 [====================>.........] - ETA: 0s - loss: 91.8388 - accuracy: 0.3655
5376/7185 [=====================>........] - ETA: 0s - loss: 86.5784 - accuracy: 0.3637
5696/7185 [======================>.......] - ETA: 0s - loss: 83.5517 - accuracy: 0.3604
6016/7185 [========================>.....] - ETA: 0s - loss: 79.5827 - accuracy: 0.3605
6304/7185 [=========================>....] - ETA: 0s - loss: 76.5201 - accuracy: 0.3598
6624/7185 [==========================>...] - ETA: 0s - loss: 75.0692 - accuracy: 0.3595
6944/7185 [===========================>..] - ETA: 0s - loss: 72.0960 - accuracy: 0.3584
7185/7185 [==============================] - 2s 213us/step - loss: 69.9736 - accuracy: 0.3603 - val_loss: 15.4447 - val_accuracy: 0.3567

Umlaut results:
[<Warning: Learning Rate is high>]
Epoch 2/3

  32/7185 [..............................] - ETA: 1s - loss: 2.4455 - accuracy: 0.3125
 384/7185 [>.............................] - ETA: 1s - loss: 6.7352 - accuracy: 0.2812
 736/7185 [==>...........................] - ETA: 1s - loss: 26.6417 - accuracy: 0.3166
1056/7185 [===>..........................] - ETA: 0s - loss: 19.9407 - accuracy: 0.3239
1408/7185 [====>.........................] - ETA: 0s - loss: 20.8685 - accuracy: 0.3239
1696/7185 [======>.......................] - ETA: 0s - loss: 24.2356 - accuracy: 0.3278
1984/7185 [=======>......................] - ETA: 0s - loss: 25.7195 - accuracy: 0.3281
2304/7185 [========>.....................] - ETA: 0s - loss: 34.2122 - accuracy: 0.3286
2656/7185 [==========>...................] - ETA: 0s - loss: 32.4982 - accuracy: 0.3321
2976/7185 [===========>..................] - ETA: 0s - loss: 36.7359 - accuracy: 0.3350
3296/7185 [============>.................] - ETA: 0s - loss: 36.6844 - accuracy: 0.3353
3648/7185 [==============>...............] - ETA: 0s - loss: 35.9454 - accuracy: 0.3369
3968/7185 [===============>..............] - ETA: 0s - loss: 35.4775 - accuracy: 0.3367
4288/7185 [================>.............] - ETA: 0s - loss: 38.4276 - accuracy: 0.3412
4576/7185 [==================>...........] - ETA: 0s - loss: 37.2786 - accuracy: 0.3424
4896/7185 [===================>..........] - ETA: 0s - loss: 35.4958 - accuracy: 0.3470
5248/7185 [====================>.........] - ETA: 0s - loss: 35.3370 - accuracy: 0.3485
5600/7185 [======================>.......] - ETA: 0s - loss: 38.3865 - accuracy: 0.3500
5920/7185 [=======================>......] - ETA: 0s - loss: 36.4501 - accuracy: 0.3458
6208/7185 [========================>.....] - ETA: 0s - loss: 36.9803 - accuracy: 0.3454
6528/7185 [==========================>...] - ETA: 0s - loss: 36.7218 - accuracy: 0.3474
6880/7185 [===========================>..] - ETA: 0s - loss: 37.2135 - accuracy: 0.3455
7185/7185 [==============================] - 1s 189us/step - loss: 38.7761 - accuracy: 0.3452 - val_loss: 123.6314 - val_accuracy: 0.3695

Umlaut results:
[<Warning: Learning Rate is high>, <Warning: Possible overfitting>]
Epoch 3/3

  32/7185 [..............................] - ETA: 1s - loss: 17.4590 - accuracy: 0.3438
 352/7185 [>.............................] - ETA: 1s - loss: 81.3218 - accuracy: 0.3722
 704/7185 [=>............................] - ETA: 1s - loss: 55.1387 - accuracy: 0.3594
1056/7185 [===>..........................] - ETA: 0s - loss: 40.0720 - accuracy: 0.3655
1376/7185 [====>.........................] - ETA: 0s - loss: 32.2082 - accuracy: 0.3706
1664/7185 [=====>........................] - ETA: 0s - loss: 28.3334 - accuracy: 0.3552
1952/7185 [=======>......................] - ETA: 0s - loss: 27.1872 - accuracy: 0.3637
2272/7185 [========>.....................] - ETA: 0s - loss: 23.9180 - accuracy: 0.3658
2592/7185 [=========>....................] - ETA: 0s - loss: 21.4458 - accuracy: 0.3638
2912/7185 [===========>..................] - ETA: 0s - loss: 19.3865 - accuracy: 0.3551
3232/7185 [============>.................] - ETA: 0s - loss: 17.7338 - accuracy: 0.3506
3552/7185 [=============>................] - ETA: 0s - loss: 20.1517 - accuracy: 0.3463
3872/7185 [===============>..............] - ETA: 0s - loss: 20.4399 - accuracy: 0.3466
4224/7185 [================>.............] - ETA: 0s - loss: 20.4801 - accuracy: 0.3478
4544/7185 [=================>............] - ETA: 0s - loss: 19.2088 - accuracy: 0.3508
4864/7185 [===================>..........] - ETA: 0s - loss: 18.1198 - accuracy: 0.3532
5216/7185 [====================>.........] - ETA: 0s - loss: 17.2422 - accuracy: 0.3470
5568/7185 [======================>.......] - ETA: 0s - loss: 16.3176 - accuracy: 0.3499
5888/7185 [=======================>......] - ETA: 0s - loss: 16.5097 - accuracy: 0.3536
6208/7185 [========================>.....] - ETA: 0s - loss: 19.1708 - accuracy: 0.3550
6528/7185 [==========================>...] - ETA: 0s - loss: 18.8423 - accuracy: 0.3539
6880/7185 [===========================>..] - ETA: 0s - loss: 21.0452 - accuracy: 0.3516
7185/7185 [==============================] - 1s 186us/step - loss: 20.2701 - accuracy: 0.3513 - val_loss: 13.5665 - val_accuracy: 0.3561

Umlaut results:
[<Warning: Learning Rate is high>]

  32/2246 [..............................] - ETA: 0s
 480/2246 [=====>........................] - ETA: 0s
 960/2246 [===========>..................] - ETA: 0s
1408/2246 [=================>............] - ETA: 0s
1888/2246 [========================>.....] - ETA: 0s
2246/2246 [==============================] - 0s 110us/step
Test loss: 13.262715217585237
Test accuracy: 0.3544078469276428
D:\nargiz\github\umlaut\venvUMLT\lib\site-packages\keras\datasets\reuters.py:85: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])
D:\nargiz\github\umlaut\venvUMLT\lib\site-packages\keras\datasets\reuters.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])
# of Training Samples: 8982
# of Test Samples: 2246
# of Classes: 46
the an brazil vs reuter 68 vs 2 lt index countries 18 000 reuter all number 000 lt oper site consumption 000 reuter considerably reserves 000 an 784 vs reuter holiday vs some 100 hit our forward up indexes share countries 624 writedown index tonnes income a but reuter countries 624 627 a exchange half be 9 dlrs on bond 07 said in all 100 hit our japanese indexes two as before on tonnes said oper half be in hit our japanese pct dlrs
3
[0. 1. 0. ... 0. 0. 0.]
10000
[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
46
7185
mdl model.h5
['loss', 'accuracy']
Train on 7185 samples, validate on 1797 samples
Umlaut results:
[<Critical: Missing Softmax layer before loss>, <Warning: Last model layer has nonlinear activation>]
Epoch 1/3

  32/7185 [..............................] - ETA: 16s - loss: 3.8654 - accuracy: 0.0000e+00
 288/7185 [>.............................] - ETA: 3s - loss: 798.9326 - accuracy: 0.2743   
 544/7185 [=>............................] - ETA: 2s - loss: 451.8825 - accuracy: 0.3254
 800/7185 [==>...........................] - ETA: 1s - loss: 322.0126 - accuracy: 0.3425
1056/7185 [===>..........................] - ETA: 1s - loss: 246.5507 - accuracy: 0.3674
1312/7185 [====>.........................] - ETA: 1s - loss: 212.3191 - accuracy: 0.3857
1600/7185 [=====>........................] - ETA: 1s - loss: 185.9431 - accuracy: 0.3988
1856/7185 [======>.......................] - ETA: 1s - loss: 160.9244 - accuracy: 0.4025
2176/7185 [========>.....................] - ETA: 1s - loss: 138.0690 - accuracy: 0.3994
2496/7185 [=========>....................] - ETA: 1s - loss: 122.0336 - accuracy: 0.3894
2816/7185 [==========>...................] - ETA: 0s - loss: 109.5543 - accuracy: 0.3924
3136/7185 [============>.................] - ETA: 0s - loss: 98.8373 - accuracy: 0.3925 
3456/7185 [=============>................] - ETA: 0s - loss: 91.4641 - accuracy: 0.3900
3808/7185 [==============>...............] - ETA: 0s - loss: 83.8709 - accuracy: 0.3876
4128/7185 [================>.............] - ETA: 0s - loss: 77.5846 - accuracy: 0.3832
4448/7185 [=================>............] - ETA: 0s - loss: 72.1978 - accuracy: 0.3795
4768/7185 [==================>...........] - ETA: 0s - loss: 67.7405 - accuracy: 0.3769
5088/7185 [====================>.........] - ETA: 0s - loss: 63.7162 - accuracy: 0.3726
5408/7185 [=====================>........] - ETA: 0s - loss: 60.4034 - accuracy: 0.3702
5728/7185 [======================>.......] - ETA: 0s - loss: 57.1622 - accuracy: 0.3698
6048/7185 [========================>.....] - ETA: 0s - loss: 54.2741 - accuracy: 0.3690
6368/7185 [=========================>....] - ETA: 0s - loss: 52.1815 - accuracy: 0.3681
6688/7185 [==========================>...] - ETA: 0s - loss: 49.8163 - accuracy: 0.3684
7008/7185 [============================>.] - ETA: 0s - loss: 47.6740 - accuracy: 0.3666
7185/7185 [==============================] - 1s 209us/step - loss: 51.7966 - accuracy: 0.3659 - val_loss: 176.8042 - val_accuracy: 0.3984

Umlaut results:
[<Warning: Learning Rate is high>]
Epoch 2/3

  32/7185 [..............................] - ETA: 1s - loss: 296.6879 - accuracy: 0.4062
 384/7185 [>.............................] - ETA: 1s - loss: 72.8567 - accuracy: 0.3594 
 736/7185 [==>...........................] - ETA: 0s - loss: 50.9033 - accuracy: 0.3845
1088/7185 [===>..........................] - ETA: 0s - loss: 46.3169 - accuracy: 0.3787
1440/7185 [=====>........................] - ETA: 0s - loss: 55.1136 - accuracy: 0.3646
1760/7185 [======>.......................] - ETA: 0s - loss: 49.2254 - accuracy: 0.3523
2080/7185 [=======>......................] - ETA: 0s - loss: 54.9351 - accuracy: 0.3476
2400/7185 [=========>....................] - ETA: 0s - loss: 50.7143 - accuracy: 0.3500
2752/7185 [==========>...................] - ETA: 0s - loss: 55.6001 - accuracy: 0.3405
3072/7185 [===========>..................] - ETA: 0s - loss: 54.0719 - accuracy: 0.3431
3392/7185 [=============>................] - ETA: 0s - loss: 52.7978 - accuracy: 0.3358
3744/7185 [==============>...............] - ETA: 0s - loss: 50.0088 - accuracy: 0.3421
4064/7185 [===============>..............] - ETA: 0s - loss: 49.1494 - accuracy: 0.3391
4384/7185 [=================>............] - ETA: 0s - loss: 47.0884 - accuracy: 0.3424
4704/7185 [==================>...........] - ETA: 0s - loss: 46.5400 - accuracy: 0.3386
5024/7185 [===================>..........] - ETA: 0s - loss: 48.7480 - accuracy: 0.3414
5376/7185 [=====================>........] - ETA: 0s - loss: 47.2232 - accuracy: 0.3376
5696/7185 [======================>.......] - ETA: 0s - loss: 45.3325 - accuracy: 0.3394
6016/7185 [========================>.....] - ETA: 0s - loss: 43.1814 - accuracy: 0.3398
6368/7185 [=========================>....] - ETA: 0s - loss: 41.8862 - accuracy: 0.3375
6720/7185 [===========================>..] - ETA: 0s - loss: 43.1251 - accuracy: 0.3411
7072/7185 [============================>.] - ETA: 0s - loss: 41.6885 - accuracy: 0.3426
7185/7185 [==============================] - 1s 183us/step - loss: 41.9202 - accuracy: 0.3431 - val_loss: 28.0338 - val_accuracy: 0.3617

Umlaut results:
[<Warning: Learning Rate is high>]
Epoch 3/3

  32/7185 [..............................] - ETA: 1s - loss: 2.6452 - accuracy: 0.3125
 352/7185 [>.............................] - ETA: 1s - loss: 15.2387 - accuracy: 0.3097
 672/7185 [=>............................] - ETA: 1s - loss: 106.9103 - accuracy: 0.3452
1024/7185 [===>..........................] - ETA: 0s - loss: 171.2067 - accuracy: 0.3613
1376/7185 [====>.........................] - ETA: 0s - loss: 171.6057 - accuracy: 0.3539
1696/7185 [======>.......................] - ETA: 0s - loss: 145.7781 - accuracy: 0.3555
2016/7185 [=======>......................] - ETA: 0s - loss: 133.3937 - accuracy: 0.3522
2336/7185 [========>.....................] - ETA: 0s - loss: 122.3910 - accuracy: 0.3562
2656/7185 [==========>...................] - ETA: 0s - loss: 110.6616 - accuracy: 0.3475
2976/7185 [===========>..................] - ETA: 0s - loss: 116.0960 - accuracy: 0.3518
3296/7185 [============>.................] - ETA: 0s - loss: 110.7707 - accuracy: 0.3516
3616/7185 [==============>...............] - ETA: 0s - loss: 102.1784 - accuracy: 0.3501
3936/7185 [===============>..............] - ETA: 0s - loss: 123.8677 - accuracy: 0.3455
4288/7185 [================>.............] - ETA: 0s - loss: 113.9828 - accuracy: 0.3444
4608/7185 [==================>...........] - ETA: 0s - loss: 106.8169 - accuracy: 0.3468
4928/7185 [===================>..........] - ETA: 0s - loss: 107.3681 - accuracy: 0.3427
5280/7185 [=====================>........] - ETA: 0s - loss: 100.3861 - accuracy: 0.3417
5632/7185 [======================>.......] - ETA: 0s - loss: 99.2887 - accuracy: 0.3434 
5952/7185 [=======================>......] - ETA: 0s - loss: 97.9462 - accuracy: 0.3407
6272/7185 [=========================>....] - ETA: 0s - loss: 93.7393 - accuracy: 0.3412
6592/7185 [==========================>...] - ETA: 0s - loss: 89.3107 - accuracy: 0.3430
6944/7185 [===========================>..] - ETA: 0s - loss: 86.1242 - accuracy: 0.3433
7185/7185 [==============================] - 1s 185us/step - loss: 83.9066 - accuracy: 0.3438 - val_loss: 69.3616 - val_accuracy: 0.3656

Umlaut results:
[<Warning: Learning Rate is high>]

  32/2246 [..............................] - ETA: 0s
 544/2246 [======>.......................] - ETA: 0s
1056/2246 [=============>................] - ETA: 0s
1568/2246 [===================>..........] - ETA: 0s
2080/2246 [==========================>...] - ETA: 0s
2246/2246 [==============================] - 0s 104us/step
Test loss: 8.599679327605562
Test accuracy: 0.3593054413795471
D:\nargiz\github\umlaut\venvUMLT\lib\site-packages\keras\datasets\reuters.py:85: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])
D:\nargiz\github\umlaut\venvUMLT\lib\site-packages\keras\datasets\reuters.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])
# of Training Samples: 8982
# of Test Samples: 2246
# of Classes: 46
the an brazil vs reuter 68 vs 2 lt index countries 18 000 reuter all number 000 lt oper site consumption 000 reuter considerably reserves 000 an 784 vs reuter holiday vs some 100 hit our forward up indexes share countries 624 writedown index tonnes income a but reuter countries 624 627 a exchange half be 9 dlrs on bond 07 said in all 100 hit our japanese indexes two as before on tonnes said oper half be in hit our japanese pct dlrs
3
[0. 1. 0. ... 0. 0. 0.]
10000
[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
46
7185
mdl model.h5
['loss', 'accuracy']
Train on 7185 samples, validate on 1797 samples
Umlaut results:
[<Critical: Missing Softmax layer before loss>, <Warning: Last model layer has nonlinear activation>]
Epoch 1/3

  32/7185 [..............................] - ETA: 16s - loss: 3.9029 - accuracy: 0.0312
 256/7185 [>.............................] - ETA: 3s - loss: 730.9042 - accuracy: 0.3672
 512/7185 [=>............................] - ETA: 2s - loss: 401.6386 - accuracy: 0.3555
 768/7185 [==>...........................] - ETA: 1s - loss: 283.3296 - accuracy: 0.3542
1056/7185 [===>..........................] - ETA: 1s - loss: 211.5127 - accuracy: 0.3665
1312/7185 [====>.........................] - ETA: 1s - loss: 174.4235 - accuracy: 0.3895
1568/7185 [=====>........................] - ETA: 1s - loss: 152.0641 - accuracy: 0.4031
1888/7185 [======>.......................] - ETA: 1s - loss: 131.2320 - accuracy: 0.3978
2208/7185 [========>.....................] - ETA: 1s - loss: 116.9652 - accuracy: 0.3995
2496/7185 [=========>....................] - ETA: 1s - loss: 104.2303 - accuracy: 0.4014
2816/7185 [==========>...................] - ETA: 0s - loss: 96.8429 - accuracy: 0.4041 
3136/7185 [============>.................] - ETA: 0s - loss: 91.7042 - accuracy: 0.4066
3488/7185 [=============>................] - ETA: 0s - loss: 85.9825 - accuracy: 0.4002
3808/7185 [==============>...............] - ETA: 0s - loss: 80.1356 - accuracy: 0.3997
4128/7185 [================>.............] - ETA: 0s - loss: 74.1841 - accuracy: 0.4000
4448/7185 [=================>............] - ETA: 0s - loss: 71.7097 - accuracy: 0.3982
4800/7185 [===================>..........] - ETA: 0s - loss: 67.4292 - accuracy: 0.3952
5120/7185 [====================>.........] - ETA: 0s - loss: 63.7662 - accuracy: 0.3934
5440/7185 [=====================>........] - ETA: 0s - loss: 60.8802 - accuracy: 0.3910
5760/7185 [=======================>......] - ETA: 0s - loss: 57.8370 - accuracy: 0.3911
6080/7185 [========================>.....] - ETA: 0s - loss: 55.1846 - accuracy: 0.3885
6432/7185 [=========================>....] - ETA: 0s - loss: 53.8903 - accuracy: 0.3863
6784/7185 [===========================>..] - ETA: 0s - loss: 59.9730 - accuracy: 0.3847
7104/7185 [============================>.] - ETA: 0s - loss: 62.9648 - accuracy: 0.3818
7185/7185 [==============================] - 2s 210us/step - loss: 62.3767 - accuracy: 0.3820 - val_loss: 11.4051 - val_accuracy: 0.3767

Umlaut results:
[<Warning: Learning Rate is high>]
Epoch 2/3

  32/7185 [..............................] - ETA: 1s - loss: 1.8930 - accuracy: 0.5312
 352/7185 [>.............................] - ETA: 1s - loss: 68.2041 - accuracy: 0.3580
 672/7185 [=>............................] - ETA: 1s - loss: 45.1791 - accuracy: 0.3810
1024/7185 [===>..........................] - ETA: 0s - loss: 59.2493 - accuracy: 0.3740
1376/7185 [====>.........................] - ETA: 0s - loss: 52.3910 - accuracy: 0.3525
1696/7185 [======>.......................] - ETA: 0s - loss: 48.7675 - accuracy: 0.3597
2016/7185 [=======>......................] - ETA: 0s - loss: 43.1010 - accuracy: 0.3571
2336/7185 [========>.....................] - ETA: 0s - loss: 41.6425 - accuracy: 0.3553
2688/7185 [==========>...................] - ETA: 0s - loss: 42.5890 - accuracy: 0.3482
3008/7185 [===========>..................] - ETA: 0s - loss: 39.0448 - accuracy: 0.3457
3296/7185 [============>.................] - ETA: 0s - loss: 35.8790 - accuracy: 0.3374
3616/7185 [==============>...............] - ETA: 0s - loss: 33.4671 - accuracy: 0.3390
3968/7185 [===============>..............] - ETA: 0s - loss: 31.5395 - accuracy: 0.3382
4288/7185 [================>.............] - ETA: 0s - loss: 31.3803 - accuracy: 0.3354
4576/7185 [==================>...........] - ETA: 0s - loss: 29.8552 - accuracy: 0.3341
4896/7185 [===================>..........] - ETA: 0s - loss: 30.9994 - accuracy: 0.3362
5216/7185 [====================>.........] - ETA: 0s - loss: 30.7458 - accuracy: 0.3376
5536/7185 [======================>.......] - ETA: 0s - loss: 29.5039 - accuracy: 0.3418
5856/7185 [=======================>......] - ETA: 0s - loss: 39.8990 - accuracy: 0.3432
6144/7185 [========================>.....] - ETA: 0s - loss: 45.7366 - accuracy: 0.3407
6464/7185 [=========================>....] - ETA: 0s - loss: 44.2723 - accuracy: 0.3399
6816/7185 [===========================>..] - ETA: 0s - loss: 42.6937 - accuracy: 0.3404
7168/7185 [============================>.] - ETA: 0s - loss: 41.2821 - accuracy: 0.3408
7185/7185 [==============================] - 1s 187us/step - loss: 41.1912 - accuracy: 0.3410 - val_loss: 9.0964 - val_accuracy: 0.3645

Umlaut results:
[<Warning: Learning Rate is high>]
Epoch 3/3

  32/7185 [..............................] - ETA: 1s - loss: 2.7980 - accuracy: 0.3438
 352/7185 [>.............................] - ETA: 1s - loss: 3.8515 - accuracy: 0.3011
 640/7185 [=>............................] - ETA: 1s - loss: 3.2857 - accuracy: 0.3078
 992/7185 [===>..........................] - ETA: 1s - loss: 40.6901 - accuracy: 0.3044
1312/7185 [====>.........................] - ETA: 0s - loss: 48.6817 - accuracy: 0.3095
1632/7185 [=====>........................] - ETA: 0s - loss: 44.4067 - accuracy: 0.3039
1920/7185 [=======>......................] - ETA: 0s - loss: 51.9316 - accuracy: 0.3109
2272/7185 [========>.....................] - ETA: 0s - loss: 45.5299 - accuracy: 0.3143
2624/7185 [=========>....................] - ETA: 0s - loss: 54.2175 - accuracy: 0.3159
2944/7185 [===========>..................] - ETA: 0s - loss: 49.6479 - accuracy: 0.3169
3264/7185 [============>.................] - ETA: 0s - loss: 46.4047 - accuracy: 0.3153
3584/7185 [=============>................] - ETA: 0s - loss: 43.2887 - accuracy: 0.3158
3904/7185 [===============>..............] - ETA: 0s - loss: 53.5089 - accuracy: 0.3199
4224/7185 [================>.............] - ETA: 0s - loss: 71.8093 - accuracy: 0.3179
4544/7185 [=================>............] - ETA: 0s - loss: 68.4028 - accuracy: 0.3145
4896/7185 [===================>..........] - ETA: 0s - loss: 67.8129 - accuracy: 0.3180
5248/7185 [====================>.........] - ETA: 0s - loss: 64.7863 - accuracy: 0.3224
5600/7185 [======================>.......] - ETA: 0s - loss: 61.7415 - accuracy: 0.3257
5920/7185 [=======================>......] - ETA: 0s - loss: 61.9317 - accuracy: 0.3262
6240/7185 [=========================>....] - ETA: 0s - loss: 59.3193 - accuracy: 0.3285
6528/7185 [==========================>...] - ETA: 0s - loss: 57.1722 - accuracy: 0.3312
6880/7185 [===========================>..] - ETA: 0s - loss: 58.1637 - accuracy: 0.3315
7185/7185 [==============================] - 1s 189us/step - loss: 55.8040 - accuracy: 0.3336 - val_loss: 11.5253 - val_accuracy: 0.3617

Umlaut results:
[<Warning: Learning Rate is high>]

  32/2246 [..............................] - ETA: 0s
 544/2246 [======>.......................] - ETA: 0s
1056/2246 [=============>................] - ETA: 0s
1504/2246 [===================>..........] - ETA: 0s
1984/2246 [=========================>....] - ETA: 0s
2246/2246 [==============================] - 0s 106us/step
Test loss: 6.742892508196809
Test accuracy: 0.35707923769950867

Process finished with exit code 0
